conv1 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
layer1.0.conv1 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer1.0.conv2 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer1.1.conv1 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer1.1.conv2 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer2.0.conv1 Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
layer2.0.conv2 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer2.0.downsample.0 Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
layer2.1.conv1 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer2.1.conv2 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer3.0.conv1 Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
layer3.0.conv2 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer3.0.downsample.0 Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
layer3.1.conv1 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer3.1.conv2 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer4.0.conv1 Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
layer4.0.conv2 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer4.0.downsample.0 Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
layer4.1.conv1 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer4.1.conv2 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
fc Linear(in_features=512, out_features=1000, bias=True)
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
Sequential(
  (0): BasicBlock(
    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (1): BasicBlock(
    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
BasicBlock(
  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
BasicBlock(
  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Sequential(
  (0): BasicBlock(
    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (downsample): Sequential(
      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
BasicBlock(
  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (downsample): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
Sequential(
  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
BasicBlock(
  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Sequential(
  (0): BasicBlock(
    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (downsample): Sequential(
      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
BasicBlock(
  (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (downsample): Sequential(
    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
Sequential(
  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
BasicBlock(
  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Sequential(
  (0): BasicBlock(
    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (downsample): Sequential(
      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
BasicBlock(
  (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (downsample): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
Sequential(
  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
BasicBlock(
  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Linear(in_features=512, out_features=1000, bias=True)
21 21 21 21 21
21

 === Layer 0 : conv1 ===
torch.Size([3, 224, 224])
torch.Size([147, 11881])
torch.Size([743, 16, 147])
> step0: Inp | [743, 1, 16, 147]
> step1: torch.Size([64, 147]) tensor(0.1999)
> step2: torch.Size([4, 16, 147])
> step3: Wgt | [[[16, 47]], [[16, 32]], [[16, 36]], [[16, 31]]]
>>> Packed: 588 to 146
[743, 1, 16, 147] 4 1 [[16, 47]]
136023.0 ->  0.061548868778280544 ms

 === Layer 1 : layer1.0.conv1 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) tensor(0.1992)
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 161]], [[16, 120]], [[16, 121]], [[16, 117]]]
>>> Packed: 2304 to 519
[183, 1, 16, 576] 4 1 [[16, 161]]
107012.0 ->  0.048421719457013575 ms

 === Layer 2 : layer1.0.conv2 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) tensor(0.1999)
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 148]], [[16, 136]], [[16, 125]], [[16, 133]]]
>>> Packed: 2304 to 542
[183, 1, 16, 576] 4 1 [[16, 148]]
106113.0 ->  0.04801493212669684 ms

 === Layer 3 : layer1.1.conv1 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) tensor(0.1997)
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 145]], [[16, 136]], [[16, 132]], [[16, 135]]]
>>> Packed: 2304 to 548
[183, 1, 16, 576] 4 1 [[16, 145]]
106115.0 ->  0.0480158371040724 ms

 === Layer 4 : layer1.1.conv2 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) tensor(0.1996)
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 142]], [[16, 135]], [[16, 135]], [[16, 133]]]
>>> Packed: 2304 to 545
[183, 1, 16, 576] 4 1 [[16, 142]]
105381.0 ->  0.04768371040723982 ms

 === Layer 5 : layer2.0.conv1 ===
torch.Size([64, 56, 56])
torch.Size([576, 729])
torch.Size([46, 16, 576])
> step0: Inp | [46, 1, 16, 576]
> step1: torch.Size([128, 576]) tensor(0.1992)
> step2: torch.Size([8, 16, 576])
> step3: Wgt | [[[16, 140]], [[16, 133]], [[16, 133]], [[16, 129]], [[16, 132]], [[16, 134]], [[16, 136]], [[16, 133]]]
>>> Packed: 4608 to 1070
[46, 1, 16, 576] 8 1 [[16, 140]]
51952.0 ->  0.02350769230769231 ms

 === Layer 6 : layer2.0.conv2 ===
torch.Size([128, 28, 28])
torch.Size([1152, 676])
torch.Size([43, 16, 1152])
> step0: Inp | [43, 2, 16, 1024]
> step1: torch.Size([128, 1152]) tensor(0.1991)
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 277]], [[16, 272]], [[16, 265]], [[16, 267]], [[16, 269]], [[16, 269]], [[16, 262]], [[16, 262]]]
>>> Packed: 9216 to 2143
[43, 2, 16, 1024] 8 1 [[16, 277]]
95169.0 ->  0.04306289592760181 ms

 === Layer 7 : layer2.0.downsample.0 ===
torch.Size([64, 56, 56])
torch.Size([64, 784])
torch.Size([49, 16, 64])
> step0: Inp | [49, 1, 16, 64]
> step1: torch.Size([128, 64]) tensor(0.1997)
> step2: torch.Size([8, 16, 64])
> step3: Wgt | [[[16, 20]], [[16, 16]], [[16, 15]], [[16, 15]], [[16, 15]], [[16, 15]], [[16, 15]], [[16, 16]]]
>>> Packed: 512 to 127
[49, 1, 16, 64] 8 1 [[16, 20]]
9003.0 ->  0.004073755656108597 ms

 === Layer 8 : layer2.1.conv1 ===
torch.Size([128, 28, 28])
torch.Size([1152, 676])
torch.Size([43, 16, 1152])
> step0: Inp | [43, 2, 16, 1024]
> step1: torch.Size([128, 1152]) tensor(0.1995)
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 277]], [[16, 271]], [[16, 262]], [[16, 265]], [[16, 269]], [[16, 269]], [[16, 268]], [[16, 273]]]
>>> Packed: 9216 to 2154
[43, 2, 16, 1024] 8 1 [[16, 277]]
95306.0 ->  0.043124886877828054 ms

 === Layer 9 : layer2.1.conv2 ===
torch.Size([128, 28, 28])
torch.Size([1152, 676])
torch.Size([43, 16, 1152])
> step0: Inp | [43, 2, 16, 1024]
> step1: torch.Size([128, 1152]) tensor(0.1998)
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 272]], [[16, 271]], [[16, 263]], [[16, 271]], [[16, 273]], [[16, 260]], [[16, 270]], [[16, 268]]]
>>> Packed: 9216 to 2148
[43, 2, 16, 1024] 8 1 [[16, 272]]
95215.0 ->  0.04308371040723982 ms

 === Layer 10 : layer3.0.conv1 ===
torch.Size([128, 28, 28])
torch.Size([1152, 169])
torch.Size([11, 16, 1152])
> step0: Inp | [11, 2, 16, 1024]
> step1: torch.Size([256, 1152]) tensor(0.1992)
> step2: torch.Size([16, 16, 1152])
> step3: Wgt | [[[16, 280]], [[16, 273]], [[16, 269]], [[16, 263]], [[16, 265]], [[16, 266]], [[16, 268]], [[16, 261]], [[16, 268]], [[16, 270]], [[16, 258]], [[16, 269]], [[16, 269]], [[16, 266]], [[16, 273]], [[16, 265]]]
>>> Packed: 18432 to 4283
[11, 2, 16, 1024] 16 1 [[16, 280]]
48748.0 ->  0.0220579185520362 ms

 === Layer 11 : layer3.0.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) tensor(0.1988)
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 551]], [[16, 541]], [[16, 536]], [[16, 535]], [[16, 533]], [[16, 541]], [[16, 548]], [[16, 529]], [[16, 542]], [[16, 524]], [[16, 529]], [[16, 522]], [[16, 533]], [[16, 537]], [[16, 541]], [[16, 535]]]
>>> Packed: 36864 to 8577
[9, 3, 16, 1024] 16 1 [[16, 551]]
78922.0 ->  0.03571131221719457 ms

 === Layer 12 : layer3.0.downsample.0 ===
torch.Size([128, 28, 28])
torch.Size([128, 196])
torch.Size([13, 16, 128])
> step0: Inp | [13, 1, 16, 128]
> step1: torch.Size([256, 128]) tensor(0.1991)
> step2: torch.Size([16, 16, 128])
> step3: Wgt | [[[16, 34]], [[16, 30]], [[16, 31]], [[16, 31]], [[16, 31]], [[16, 31]], [[16, 31]], [[16, 31]], [[16, 32]], [[16, 28]], [[16, 31]], [[16, 28]], [[16, 30]], [[16, 29]], [[16, 31]], [[16, 29]]]
>>> Packed: 2048 to 488
[13, 1, 16, 128] 16 1 [[16, 34]]
7850.0 ->  0.0035520361990950228 ms

 === Layer 13 : layer3.1.conv1 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) tensor(0.1999)
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 553]], [[16, 546]], [[16, 535]], [[16, 532]], [[16, 532]], [[16, 531]], [[16, 531]], [[16, 538]], [[16, 534]], [[16, 540]], [[16, 541]], [[16, 538]], [[16, 538]], [[16, 569]], [[16, 537]], [[16, 543]]]
>>> Packed: 36864 to 8638
[9, 3, 16, 1024] 16 1 [[16, 553]]
79542.0 ->  0.03599185520361991 ms

 === Layer 14 : layer3.1.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) tensor(0.1996)
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 585]], [[16, 551]], [[16, 508]], [[16, 544]], [[16, 514]], [[16, 536]], [[16, 526]], [[16, 534]], [[16, 527]], [[16, 526]], [[16, 537]], [[16, 545]], [[16, 539]], [[16, 531]], [[16, 522]], [[16, 532]]]
>>> Packed: 36864 to 8557
[9, 3, 16, 1024] 16 1 [[16, 585]]
79450.0 ->  0.035950226244343894 ms

 === Layer 15 : layer4.0.conv1 ===
torch.Size([256, 14, 14])
torch.Size([2304, 36])
torch.Size([3, 16, 2304])
> step0: Inp | [3, 3, 16, 1024]
> step1: torch.Size([512, 2304]) tensor(0.1996)
> step2: torch.Size([32, 16, 2304])
> step3: Wgt | [[[16, 559]], [[16, 540]], [[16, 539]], [[16, 536]], [[16, 534]], [[16, 540]], [[16, 533]], [[16, 536]], [[16, 534]], [[16, 522]], [[16, 535]], [[16, 527]], [[16, 527]], [[16, 533]], [[16, 532]], [[16, 535]], [[16, 536]], [[16, 538]], [[16, 536]], [[16, 533]], [[16, 537]], [[16, 532]], [[16, 529]], [[16, 530]], [[16, 545]], [[16, 532]], [[16, 537]], [[16, 539]], [[16, 538]], [[16, 540]], [[16, 543]], [[16, 531]]]
>>> Packed: 73728 to 17138
[3, 3, 16, 1024] 32 1 [[16, 559]]
52728.0 ->  0.023858823529411766 ms

 === Layer 16 : layer4.0.conv2 ===
torch.Size([512, 7, 7])
torch.Size([4608, 25])
torch.Size([2, 16, 4608])
> step0: Inp | [2, 5, 16, 1024]
> step1: torch.Size([512, 4608]) tensor(0.1997)
> step2: torch.Size([32, 16, 4608])
> step3: Wgt | [[[16, 1024], [16, 91]], [[16, 1024], [16, 58]], [[16, 1024], [16, 55]], [[16, 1024], [16, 45]], [[16, 1024], [16, 42]], [[16, 1024], [16, 57]], [[16, 1024], [16, 43]], [[16, 1024], [16, 43]], [[16, 1024], [16, 41]], [[16, 1024], [16, 49]], [[16, 1024], [16, 47]], [[16, 1024], [16, 40]], [[16, 1024], [16, 39]], [[16, 1024], [16, 44]], [[16, 1024], [16, 71]], [[16, 1024], [16, 54]], [[16, 1024], [16, 58]], [[16, 1024], [16, 59]], [[16, 1024], [16, 42]], [[16, 1024], [16, 44]], [[16, 1024], [16, 42]], [[16, 1024], [16, 61]], [[16, 1024], [16, 54]], [[16, 1024], [16, 47]], [[16, 1024], [16, 37]], [[16, 1024], [16, 47]], [[16, 1024], [16, 50]], [[16, 1024], [16, 44]], [[16, 1024], [16, 48]], [[16, 1024], [16, 51]], [[16, 1024], [16, 38]], [[16, 1024], [16, 42]]]
>>> Packed: 147456 to 32768
[2, 5, 16, 1024] 32 2 [[16, 1024], [16, 91]]
131568.0 ->  0.05953303167420815 ms

 === Layer 17 : layer4.0.downsample.0 ===
torch.Size([256, 14, 14])
torch.Size([256, 49])
torch.Size([4, 16, 256])
> step0: Inp | [4, 1, 16, 256]
> step1: torch.Size([512, 256]) tensor(0.1995)
> step2: torch.Size([32, 16, 256])
> step3: Wgt | [[[16, 61]], [[16, 61]], [[16, 60]], [[16, 59]], [[16, 60]], [[16, 59]], [[16, 60]], [[16, 60]], [[16, 56]], [[16, 60]], [[16, 60]], [[16, 59]], [[16, 58]], [[16, 60]], [[16, 59]], [[16, 59]], [[16, 59]], [[16, 58]], [[16, 58]], [[16, 59]], [[16, 59]], [[16, 55]], [[16, 60]], [[16, 59]], [[16, 59]], [[16, 60]], [[16, 57]], [[16, 61]], [[16, 59]], [[16, 60]], [[16, 58]], [[16, 60]]]
>>> Packed: 8192 to 1892
[4, 1, 16, 256] 32 1 [[16, 61]]
8541.0 ->  0.0038647058823529415 ms

 === Layer 18 : layer4.1.conv1 ===
torch.Size([512, 7, 7])
torch.Size([4608, 25])
torch.Size([2, 16, 4608])
> step0: Inp | [2, 5, 16, 1024]
> step1: torch.Size([512, 4608]) tensor(0.1994)
> step2: torch.Size([32, 16, 4608])
> step3: Wgt | [[[16, 1024], [16, 63]], [[16, 1024], [16, 52]], [[16, 1024], [16, 41]], [[16, 1024], [16, 33]], [[16, 1024], [16, 21]], [[16, 1024], [16, 24]], [[16, 1024], [16, 28]], [[16, 1024], [16, 59]], [[16, 1024], [16, 44]], [[16, 1024], [16, 25]], [[16, 1024], [16, 36]], [[16, 1024], [16, 30]], [[16, 1024], [16, 36]], [[16, 1024], [16, 36]], [[16, 1024], [16, 24]], [[16, 1024], [16, 39]], [[16, 1024], [16, 40]], [[16, 1024], [16, 38]], [[16, 1024], [16, 20]], [[16, 1024], [16, 34]], [[16, 1024], [16, 31]], [[16, 1024], [16, 38]], [[16, 1024], [16, 24]], [[16, 1024], [16, 34]], [[16, 1024], [16, 44]], [[16, 1024], [16, 48]], [[16, 1024], [16, 31]], [[16, 1024], [16, 39]], [[16, 1024], [16, 39]], [[16, 1024], [16, 31]], [[16, 1024], [16, 32]], [[16, 1024], [16, 27]]]
>>> Packed: 147456 to 32768
[2, 5, 16, 1024] 32 2 [[16, 1024], [16, 63]]
131553.0 ->  0.05952624434389141 ms

 === Layer 19 : layer4.1.conv2 ===
torch.Size([512, 7, 7])
torch.Size([4608, 25])
torch.Size([2, 16, 4608])
> step0: Inp | [2, 5, 16, 1024]
> step1: torch.Size([512, 4608]) tensor(0.1987)
> step2: torch.Size([32, 16, 4608])
> step3: Wgt | [[[16, 1024], [16, 91]], [[16, 1024], [16, 68]], [[16, 1024], [16, 60]], [[16, 1024], [16, 37]], [[16, 1024], [16, 41]], [[16, 1024], [16, 44]], [[16, 1024], [16, 49]], [[16, 1024], [16, 43]], [[16, 1024], [16, 36]], [[16, 1024], [16, 41]], [[16, 1024], [16, 44]], [[16, 1024], [16, 34]], [[16, 1024], [16, 47]], [[16, 1024], [16, 37]], [[16, 1024], [16, 37]], [[16, 1024], [16, 49]], [[16, 1024], [16, 34]], [[16, 1024], [16, 34]], [[16, 1024], [16, 41]], [[16, 1024], [16, 50]], [[16, 1024], [16, 49]], [[16, 1024], [16, 32]], [[16, 1024], [16, 43]], [[16, 1024], [16, 38]], [[16, 1024], [16, 50]], [[16, 1024], [16, 40]], [[16, 1024], [16, 45]], [[16, 1024], [16, 37]], [[16, 1024], [16, 25]], [[16, 1024], [16, 45]], [[16, 1024], [16, 35]], [[16, 1024], [16, 61]]]
>>> Packed: 147456 to 32768
[2, 5, 16, 1024] 32 2 [[16, 1024], [16, 91]]
131587.0 ->  0.05954162895927602 ms

 === Layer 20 : fc ===
torch.Size([512])
torch.Size([512, 1])
torch.Size([1, 16, 512])
> step0: Inp | [1, 1, 16, 512]
> step1: torch.Size([1000, 512]) tensor(0.1997)
> step2: torch.Size([63, 16, 512])
> step3: Wgt | [[[16, 127]], [[16, 117]], [[16, 102]], [[16, 106]], [[16, 106]], [[16, 108]], [[16, 120]], [[16, 113]], [[16, 111]], [[16, 120]], [[16, 102]], [[16, 109]], [[16, 105]], [[16, 106]], [[16, 105]], [[16, 104]], [[16, 107]], [[16, 112]], [[16, 109]], [[16, 107]], [[16, 112]], [[16, 115]], [[16, 110]], [[16, 104]], [[16, 119]], [[16, 117]], [[16, 119]], [[16, 117]], [[16, 117]], [[16, 121]], [[16, 116]], [[16, 119]], [[16, 115]], [[16, 118]], [[16, 119]], [[16, 118]], [[16, 122]], [[16, 118]], [[16, 118]], [[16, 120]], [[16, 119]], [[16, 115]], [[16, 120]], [[16, 120]], [[16, 119]], [[16, 121]], [[16, 120]], [[16, 120]], [[16, 121]], [[16, 118]], [[16, 119]], [[16, 120]], [[16, 118]], [[16, 118]], [[16, 114]], [[16, 121]], [[16, 117]], [[16, 116]], [[16, 104]], [[16, 104]], [[16, 111]], [[16, 112]], [[16, 468]]]
>>> Packed: 32256 to 7565
[1, 1, 16, 512] 63 1 [[16, 127]]
8513.0 ->  0.0038520361990950227 ms
The overall latecy is: 0.7539778280542987

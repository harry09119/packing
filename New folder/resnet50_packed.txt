<Resnet50 with 80% Sparsity with base mode SA, mux size 4>

 === Layer 0 : conv1 ===
torch.Size([3, 224, 224])
torch.Size([147, 11881])
torch.Size([743, 16, 147])
> step0: Inp | [743, 1, 16, 147]
> step1: torch.Size([64, 147]) tensor(0.1994)
> step2: torch.Size([4, 16, 147])
> step3: Wgt | [[[16, 44]], [[16, 36]], [[16, 34]], [[16, 29]]]
>>> Packed: 588 to 143
[743, 1, 16, 147] 4 1 [[16, 44]]
INFO:root:Running MatMul
127080.0 ->  0.057502262443438916 ms

 === Layer 1 : layer1.0.conv1 ===
torch.Size([64, 56, 56])
torch.Size([64, 3136])
torch.Size([196, 16, 64])
> step0: Inp | [196, 1, 16, 64]
> step1: torch.Size([64, 64]) tensor(0.1990)
> step2: torch.Size([4, 16, 64])
> step3: Wgt | [[[16, 26]], [[16, 14]], [[16, 15]], [[16, 16]]]
>>> Packed: 256 to 71
[196, 1, 16, 64] 4 1 [[16, 26]]
INFO:root:Running MatMul
19427.0 ->  0.008790497737556562 ms

 === Layer 2 : layer1.0.conv2 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) tensor(0.1991)
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 137]], [[16, 131]], [[16, 144]], [[16, 131]]]
>>> Packed: 2304 to 543
[183, 1, 16, 576] 4 1 [[16, 137]]
INFO:root:Running MatMul
104544.0 ->  0.04730497737556561 ms

 === Layer 3 : layer1.0.conv3 ===
torch.Size([64, 56, 56])
torch.Size([64, 3136])
torch.Size([196, 16, 64])
> step0: Inp | [196, 1, 16, 64]
> step1: torch.Size([256, 64]) tensor(0.1998)
> step2: torch.Size([16, 16, 64])
> step3: Wgt | [[[16, 20]], [[16, 14]], [[16, 16]], [[16, 16]], [[16, 14]], [[16, 15]], [[16, 14]], [[16, 15]], [[16, 16]], [[16, 14]], [[16, 16]], [[16, 16]], [[16, 13]], [[16, 15]], [[16, 15]], [[16, 15]]]
>>> Packed: 1024 to 244
[196, 1, 16, 64] 16 1 [[16, 20]]
INFO:root:Running MatMul
69797.0 ->  0.03158235294117647 ms

 === Layer 4 : layer1.0.downsample.0 ===
torch.Size([64, 56, 56])
torch.Size([64, 3136])
torch.Size([196, 16, 64])
> step0: Inp | [196, 1, 16, 64]
> step1: torch.Size([256, 64]) tensor(0.1996)
> step2: torch.Size([16, 16, 64])
> step3: Wgt | [[[16, 18]], [[16, 14]], [[16, 13]], [[16, 14]], [[16, 12]], [[16, 18]], [[16, 15]], [[16, 16]], [[16, 16]], [[16, 14]], [[16, 28]], [[16, 12]], [[16, 13]], [[16, 16]], [[16, 15]], [[16, 16]]]
>>> Packed: 1024 to 250
[196, 1, 16, 64] 16 1 [[16, 18]]
INFO:root:Running MatMul
70973.0 ->  0.03211447963800905 ms

 === Layer 5 : layer1.1.conv1 ===
torch.Size([256, 56, 56])
torch.Size([256, 3136])
torch.Size([196, 16, 256])
> step0: Inp | [196, 1, 16, 256]
> step1: torch.Size([64, 256]) tensor(0.2000)
> step2: torch.Size([4, 16, 256])
> step3: Wgt | [[[16, 67]], [[16, 55]], [[16, 54]], [[16, 57]]]
>>> Packed: 1024 to 233
[196, 1, 16, 256] 4 1 [[16, 67]]
INFO:root:Running MatMul
51189.0 ->  0.023162443438914028 ms

 === Layer 6 : layer1.1.conv2 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) tensor(0.1996)
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 146]], [[16, 134]], [[16, 129]], [[16, 129]]]
>>> Packed: 2304 to 538
[183, 1, 16, 576] 4 1 [[16, 146]]
INFO:root:Running MatMul
103631.0 ->  0.04689185520361991 ms

 === Layer 7 : layer1.1.conv3 ===
torch.Size([64, 56, 56])
torch.Size([64, 3136])
torch.Size([196, 16, 64])
> step0: Inp | [196, 1, 16, 64]
> step1: torch.Size([256, 64]) tensor(0.1997)
> step2: torch.Size([16, 16, 64])
> step3: Wgt | [[[16, 17]], [[16, 14]], [[16, 15]], [[16, 17]], [[16, 14]], [[16, 15]], [[16, 15]], [[16, 15]], [[16, 18]], [[16, 16]], [[16, 16]], [[16, 13]], [[16, 13]], [[16, 16]], [[16, 17]], [[16, 15]]]
>>> Packed: 1024 to 246
[196, 1, 16, 64] 16 1 [[16, 17]]
INFO:root:Running MatMul
70189.0 ->  0.031759728506787335 ms

 === Layer 8 : layer1.2.conv1 ===
torch.Size([256, 56, 56])
torch.Size([256, 3136])
torch.Size([196, 16, 256])
> step0: Inp | [196, 1, 16, 256]
> step1: torch.Size([64, 256]) tensor(0.1993)
> step2: torch.Size([4, 16, 256])
> step3: Wgt | [[[16, 64]], [[16, 58]], [[16, 62]], [[16, 52]]]
>>> Packed: 1024 to 236
[196, 1, 16, 256] 4 1 [[16, 64]]
INFO:root:Running MatMul
51776.0 ->  0.023428054298642535 ms

 === Layer 9 : layer1.2.conv2 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) tensor(0.1996)
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 135]], [[16, 128]], [[16, 136]], [[16, 130]]]
>>> Packed: 2304 to 529
[183, 1, 16, 576] 4 1 [[16, 135]]
INFO:root:Running MatMul
101981.0 ->  0.046145248868778285 ms

 === Layer 10 : layer1.2.conv3 ===
torch.Size([64, 56, 56])
torch.Size([64, 3136])
torch.Size([196, 16, 64])
> step0: Inp | [196, 1, 16, 64]
> step1: torch.Size([256, 64]) tensor(0.1995)
> step2: torch.Size([16, 16, 64])
> step3: Wgt | [[[16, 21]], [[16, 13]], [[16, 13]], [[16, 15]], [[16, 13]], [[16, 19]], [[16, 13]], [[16, 16]], [[16, 21]], [[16, 16]], [[16, 16]], [[16, 15]], [[16, 16]], [[16, 18]], [[16, 11]], [[16, 14]]]
>>> Packed: 1024 to 250
[196, 1, 16, 64] 16 1 [[16, 21]]
INFO:root:Running MatMul
70974.0 ->  0.03211493212669683 ms

 === Layer 11 : layer2.0.conv1 ===
torch.Size([256, 56, 56])
torch.Size([256, 3136])
torch.Size([196, 16, 256])
> step0: Inp | [196, 1, 16, 256]
> step1: torch.Size([128, 256]) tensor(0.1992)
> step2: torch.Size([8, 16, 256])
> step3: Wgt | [[[16, 75]], [[16, 56]], [[16, 59]], [[16, 56]], [[16, 59]], [[16, 54]], [[16, 55]], [[16, 58]]]
>>> Packed: 2048 to 472
[196, 1, 16, 256] 8 1 [[16, 75]]
INFO:root:Running MatMul
103523.0 ->  0.04684298642533937 ms

 === Layer 12 : layer2.0.conv2 ===
torch.Size([128, 56, 56])
torch.Size([1152, 729])
torch.Size([46, 16, 1152])
> step0: Inp | [46, 2, 16, 1024]
> step1: torch.Size([128, 1152]) tensor(0.1998)
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 279]], [[16, 270]], [[16, 266]], [[16, 266]], [[16, 272]], [[16, 266]], [[16, 269]], [[16, 270]]]
>>> Packed: 9216 to 2158
[46, 2, 16, 1024] 8 1 [[16, 279]]
INFO:root:Running MatMul
101913 ->  0.046114479638009055 ms

 === Layer 13 : layer2.0.conv3 ===
torch.Size([128, 28, 28])
torch.Size([128, 784])
torch.Size([49, 16, 128])
> step0: Inp | [49, 1, 16, 128]
> step1: torch.Size([512, 128]) tensor(0.1992)
> step2: torch.Size([32, 16, 128])
> step3: Wgt | [[[16, 32]], [[16, 29]], [[16, 32]], [[16, 29]], [[16, 31]], [[16, 30]], [[16, 30]], [[16, 30]], [[16, 31]], [[16, 28]], [[16, 31]], [[16, 38]], [[16, 32]], [[16, 27]], [[16, 34]], [[16, 30]], [[16, 33]], [[16, 29]], [[16, 38]], [[16, 21]], [[16, 39]], [[16, 27]], [[16, 30]], [[16, 31]], [[16, 30]], [[16, 31]], [[16, 28]], [[16, 32]], [[16, 31]], [[16, 28]], [[16, 29]], [[16, 31]]]
>>> Packed: 4096 to 982
[49, 1, 16, 128] 32 1 [[16, 32]]
INFO:root:Running MatMul
59118.0 ->  0.026750226244343894 ms

 === Layer 14 : layer2.0.downsample.0 ===
torch.Size([256, 56, 56])
torch.Size([256, 784])
torch.Size([49, 16, 256])
> step0: Inp | [49, 1, 16, 256]
> step1: torch.Size([512, 256]) tensor(0.1991)
> step2: torch.Size([32, 16, 256])
> step3: Wgt | [[[16, 71]], [[16, 59]], [[16, 64]], [[16, 59]], [[16, 53]], [[16, 56]], [[16, 63]], [[16, 62]], [[16, 60]], [[16, 62]], [[16, 59]], [[16, 62]], [[16, 59]], [[16, 59]], [[16, 57]], [[16, 61]], [[16, 61]], [[16, 61]], [[16, 57]], [[16, 55]], [[16, 60]], [[16, 61]], [[16, 65]], [[16, 61]], [[16, 62]], [[16, 60]], [[16, 54]], [[16, 57]], [[16, 55]], [[16, 57]], [[16, 61]], [[16, 62]]]
>>> Packed: 8192 to 1915
[49, 1, 16, 256] 32 1 [[16, 71]]
INFO:root:Running MatMul
104845.0 ->  0.04744117647058824 ms

 === Layer 15 : layer2.1.conv1 ===
torch.Size([512, 28, 28])
torch.Size([512, 784])
torch.Size([49, 16, 512])
> step0: Inp | [49, 1, 16, 512]
> step1: torch.Size([128, 512]) tensor(0.1984)
> step2: torch.Size([8, 16, 512])
> step3: Wgt | [[[16, 131]], [[16, 109]], [[16, 108]], [[16, 107]], [[16, 122]], [[16, 101]], [[16, 104]], [[16, 109]]]
>>> Packed: 4096 to 891
[49, 1, 16, 512] 8 1 [[16, 131]]
INFO:root:Running MatMul
46452.0 ->  0.021019004524886878 ms

 === Layer 16 : layer2.1.conv2 ===
torch.Size([128, 28, 28])
torch.Size([1152, 676])
torch.Size([43, 16, 1152])
> step0: Inp | [43, 2, 16, 1024]
> step1: torch.Size([128, 1152]) tensor(0.1986)
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 283]], [[16, 274]], [[16, 270]], [[16, 268]], [[16, 270]], [[16, 265]], [[16, 260]], [[16, 260]]]
>>> Packed: 9216 to 2150
[43, 2, 16, 1024] 8 1 [[16, 283]]
INFO:root:Running MatMul
94928 ->  0.042953846153846155 ms

 === Layer 17 : layer2.1.conv3 ===
torch.Size([128, 28, 28])
torch.Size([128, 784])
torch.Size([49, 16, 128])
> step0: Inp | [49, 1, 16, 128]
> step1: torch.Size([512, 128]) tensor(0.1988)
> step2: torch.Size([32, 16, 128])
> step3: Wgt | [[[16, 44]], [[16, 30]], [[16, 32]], [[16, 27]], [[16, 22]], [[16, 30]], [[16, 31]], [[16, 28]], [[16, 26]], [[16, 32]], [[16, 29]], [[16, 37]], [[16, 30]], [[16, 28]], [[16, 27]], [[16, 33]], [[16, 26]], [[16, 37]], [[16, 28]], [[16, 31]], [[16, 30]], [[16, 31]], [[16, 29]], [[16, 28]], [[16, 28]], [[16, 31]], [[16, 31]], [[16, 34]], [[16, 26]], [[16, 33]], [[16, 28]], [[16, 30]]]
>>> Packed: 4096 to 967
[49, 1, 16, 128] 32 1 [[16, 44]]
INFO:root:Running MatMul
58386.0 ->  0.02641900452488688 ms

 === Layer 18 : layer2.2.conv1 ===
torch.Size([512, 28, 28])
torch.Size([512, 784])
torch.Size([49, 16, 512])
> step0: Inp | [49, 1, 16, 512]
> step1: torch.Size([128, 512]) tensor(0.1999)
> step2: torch.Size([8, 16, 512])
> step3: Wgt | [[[16, 123]], [[16, 120]], [[16, 117]], [[16, 118]], [[16, 117]], [[16, 112]], [[16, 115]], [[16, 116]]]
>>> Packed: 4096 to 938
[49, 1, 16, 512] 8 1 [[16, 123]]
INFO:root:Running MatMul
48753.0 ->  0.022060180995475114 ms

 === Layer 19 : layer2.2.conv2 ===
torch.Size([128, 28, 28])
torch.Size([1152, 676])
torch.Size([43, 16, 1152])
> step0: Inp | [43, 2, 16, 1024]
> step1: torch.Size([128, 1152]) tensor(0.1988)
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 278]], [[16, 274]], [[16, 267]], [[16, 262]], [[16, 270]], [[16, 264]], [[16, 267]], [[16, 266]]]
>>> Packed: 9216 to 2148
[43, 2, 16, 1024] 8 1 [[16, 278]]
INFO:root:Running MatMul
94841 ->  0.042914479638009054 ms

 === Layer 20 : layer2.2.conv3 ===
torch.Size([128, 28, 28])
torch.Size([128, 784])
torch.Size([49, 16, 128])
> step0: Inp | [49, 1, 16, 128]
> step1: torch.Size([512, 128]) tensor(0.1990)
> step2: torch.Size([32, 16, 128])
> step3: Wgt | [[[16, 32]], [[16, 33]], [[16, 30]], [[16, 28]], [[16, 30]], [[16, 30]], [[16, 30]], [[16, 29]], [[16, 31]], [[16, 30]], [[16, 30]], [[16, 30]], [[16, 32]], [[16, 30]], [[16, 31]], [[16, 29]], [[16, 27]], [[16, 29]], [[16, 30]], [[16, 31]], [[16, 29]], [[16, 31]], [[16, 31]], [[16, 28]], [[16, 29]], [[16, 29]], [[16, 30]], [[16, 31]], [[16, 33]], [[16, 30]], [[16, 32]], [[16, 30]]]
>>> Packed: 4096 to 965
[49, 1, 16, 128] 32 1 [[16, 32]]
INFO:root:Running MatMul
58285.0 ->  0.026373303167420818 ms

 === Layer 21 : layer2.3.conv1 ===
torch.Size([512, 28, 28])
torch.Size([512, 784])
torch.Size([49, 16, 512])
> step0: Inp | [49, 1, 16, 512]
> step1: torch.Size([128, 512]) tensor(0.1997)
> step2: torch.Size([8, 16, 512])
> step3: Wgt | [[[16, 126]], [[16, 120]], [[16, 121]], [[16, 118]], [[16, 116]], [[16, 118]], [[16, 115]], [[16, 120]]]
>>> Packed: 4096 to 954
[49, 1, 16, 512] 8 1 [[16, 126]]
INFO:root:Running MatMul
49538.0 ->  0.02241538461538462 ms

 === Layer 22 : layer2.3.conv2 ===
torch.Size([128, 28, 28])
torch.Size([1152, 676])
torch.Size([43, 16, 1152])
> step0: Inp | [43, 2, 16, 1024]
> step1: torch.Size([128, 1152]) tensor(0.1991)
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 278]], [[16, 273]], [[16, 267]], [[16, 268]], [[16, 265]], [[16, 274]], [[16, 265]], [[16, 262]]]
>>> Packed: 9216 to 2152
[43, 2, 16, 1024] 8 1 [[16, 278]]
INFO:root:Running MatMul
95013 ->  0.0429923076923077 ms

 === Layer 23 : layer2.3.conv3 ===
torch.Size([128, 28, 28])
torch.Size([128, 784])
torch.Size([49, 16, 128])
> step0: Inp | [49, 1, 16, 128]
> step1: torch.Size([512, 128]) tensor(0.1990)
> step2: torch.Size([32, 16, 128])
> step3: Wgt | [[[16, 34]], [[16, 33]], [[16, 30]], [[16, 33]], [[16, 31]], [[16, 32]], [[16, 28]], [[16, 31]], [[16, 31]], [[16, 28]], [[16, 28]], [[16, 31]], [[16, 31]], [[16, 31]], [[16, 32]], [[16, 30]], [[16, 29]], [[16, 29]], [[16, 30]], [[16, 29]], [[16, 25]], [[16, 27]], [[16, 31]], [[16, 30]], [[16, 30]], [[16, 30]], [[16, 33]], [[16, 34]], [[16, 30]], [[16, 28]], [[16, 31]], [[16, 29]]]
>>> Packed: 4096 to 969
[49, 1, 16, 128] 32 1 [[16, 34]]
INFO:root:Running MatMul
58482.0 ->  0.02646244343891403 ms

 === Layer 24 : layer3.0.conv1 ===
torch.Size([512, 28, 28])
torch.Size([512, 784])
torch.Size([49, 16, 512])
> step0: Inp | [49, 1, 16, 512]
> step1: torch.Size([256, 512]) tensor(0.1998)
> step2: torch.Size([16, 16, 512])
> step3: Wgt | [[[16, 122]], [[16, 123]], [[16, 121]], [[16, 121]], [[16, 116]], [[16, 117]], [[16, 116]], [[16, 119]], [[16, 113]], [[16, 117]], [[16, 119]], [[16, 120]], [[16, 121]], [[16, 121]], [[16, 118]], [[16, 120]]]
>>> Packed: 8192 to 1904
[49, 1, 16, 512] 16 1 [[16, 122]]
INFO:root:Running MatMul
98831.0 ->  0.04471990950226245 ms

 === Layer 25 : layer3.0.conv2 ===
torch.Size([256, 28, 28])
torch.Size([2304, 169])
torch.Size([11, 16, 2304])
> step0: Inp | [11, 3, 16, 1024]
> step1: torch.Size([256, 2304]) tensor(0.1999)
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 583]], [[16, 547]], [[16, 540]], [[16, 544]], [[16, 544]], [[16, 533]], [[16, 516]], [[16, 529]], [[16, 545]], [[16, 537]], [[16, 541]], [[16, 542]], [[16, 545]], [[16, 541]], [[16, 537]], [[16, 521]]]
>>> Packed: 36864 to 8645
[11, 3, 16, 1024] 16 1 [[16, 583]]
INFO:root:Running MatMul
96472 ->  0.043652488687782806 ms

 === Layer 26 : layer3.0.conv3 ===
torch.Size([256, 14, 14])
torch.Size([256, 196])
torch.Size([13, 16, 256])
> step0: Inp | [13, 1, 16, 256]
> step1: torch.Size([1024, 256]) tensor(0.1994)
> step2: torch.Size([64, 16, 256])
> step3: Wgt | [[[16, 63]], [[16, 59]], [[16, 58]], [[16, 60]], [[16, 63]], [[16, 62]], [[16, 60]], [[16, 61]], [[16, 60]], [[16, 57]], [[16, 59]], [[16, 59]], [[16, 60]], [[16, 54]], [[16, 60]], [[16, 60]], [[16, 61]], [[16, 62]], [[16, 58]], [[16, 57]], [[16, 56]], [[16, 57]], [[16, 57]], [[16, 60]], [[16, 59]], [[16, 62]], [[16, 60]], [[16, 57]], [[16, 59]], [[16, 59]], [[16, 61]], [[16, 61]], [[16, 60]], [[16, 59]], [[16, 66]], [[16, 61]], [[16, 62]], [[16, 66]], [[16, 61]], [[16, 63]], [[16, 60]], [[16, 60]], [[16, 60]], [[16, 61]], [[16, 58]], [[16, 60]], [[16, 57]], [[16, 60]], [[16, 61]], [[16, 59]], [[16, 61]], [[16, 60]], [[16, 61]], [[16, 59]], [[16, 60]], [[16, 60]], [[16, 60]], [[16, 59]], [[16, 58]], [[16, 60]], [[16, 60]], [[16, 56]], [[16, 61]], [[16, 59]]]
>>> Packed: 16384 to 3829
[13, 1, 16, 256] 64 1 [[16, 63]]
INFO:root:Running MatMul
55633.0 ->  0.025173303167420814 ms

 === Layer 27 : layer3.0.downsample.0 ===
torch.Size([512, 28, 28])
torch.Size([512, 196])
torch.Size([13, 16, 512])
> step0: Inp | [13, 1, 16, 512]
> step1: torch.Size([1024, 512]) tensor(0.1986)
> step2: torch.Size([64, 16, 512])
> step3: Wgt | [[[16, 134]], [[16, 117]], [[16, 118]], [[16, 116]], [[16, 120]], [[16, 122]], [[16, 120]], [[16, 119]], [[16, 122]], [[16, 113]], [[16, 120]], [[16, 122]], [[16, 123]], [[16, 114]], [[16, 115]], [[16, 119]], [[16, 121]], [[16, 122]], [[16, 117]], [[16, 111]], [[16, 113]], [[16, 107]], [[16, 111]], [[16, 113]], [[16, 117]], [[16, 116]], [[16, 120]], [[16, 119]], [[16, 120]], [[16, 120]], [[16, 119]], [[16, 121]], [[16, 117]], [[16, 117]], [[16, 121]], [[16, 121]], [[16, 123]], [[16, 123]], [[16, 123]], [[16, 122]], [[16, 123]], [[16, 123]], [[16, 119]], [[16, 118]], [[16, 119]], [[16, 117]], [[16, 120]], [[16, 122]], [[16, 122]], [[16, 117]], [[16, 121]], [[16, 118]], [[16, 121]], [[16, 120]], [[16, 118]], [[16, 122]], [[16, 121]], [[16, 116]], [[16, 102]], [[16, 118]], [[16, 119]], [[16, 118]], [[16, 119]], [[16, 116]]]
>>> Packed: 32768 to 7597
[13, 1, 16, 512] 64 1 [[16, 134]]
INFO:root:Running MatMul
104635.0 ->  0.04734615384615385 ms

 === Layer 28 : layer3.1.conv1 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 196])
torch.Size([13, 16, 1024])
> step0: Inp | [13, 1, 16, 1024]
> step1: torch.Size([256, 1024]) tensor(0.1998)
> step2: torch.Size([16, 16, 1024])
> step3: Wgt | [[[16, 234]], [[16, 228]], [[16, 228]], [[16, 243]], [[16, 220]], [[16, 228]], [[16, 230]], [[16, 229]], [[16, 227]], [[16, 224]], [[16, 220]], [[16, 228]], [[16, 223]], [[16, 237]], [[16, 222]], [[16, 221]]]
>>> Packed: 16384 to 3642
[13, 1, 16, 1024] 16 1 [[16, 234]]
INFO:root:Running MatMul
48877.0 ->  0.022116289592760182 ms

 === Layer 29 : layer3.1.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) tensor(0.1998)
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 565]], [[16, 552]], [[16, 542]], [[16, 536]], [[16, 549]], [[16, 546]], [[16, 530]], [[16, 541]], [[16, 538]], [[16, 532]], [[16, 538]], [[16, 529]], [[16, 535]], [[16, 525]], [[16, 534]], [[16, 535]]]
>>> Packed: 36864 to 8627
[9, 3, 16, 1024] 16 1 [[16, 565]]
INFO:root:Running MatMul
78792 ->  0.035652488687782806 ms

 === Layer 30 : layer3.1.conv3 ===
torch.Size([256, 14, 14])
torch.Size([256, 196])
torch.Size([13, 16, 256])
> step0: Inp | [13, 1, 16, 256]
> step1: torch.Size([1024, 256]) tensor(0.1988)
> step2: torch.Size([64, 16, 256])
> step3: Wgt | [[[16, 63]], [[16, 62]], [[16, 60]], [[16, 60]], [[16, 61]], [[16, 62]], [[16, 61]], [[16, 61]], [[16, 63]], [[16, 60]], [[16, 64]], [[16, 59]], [[16, 59]], [[16, 59]], [[16, 56]], [[16, 58]], [[16, 57]], [[16, 60]], [[16, 55]], [[16, 57]], [[16, 57]], [[16, 58]], [[16, 59]], [[16, 60]], [[16, 59]], [[16, 58]], [[16, 57]], [[16, 61]], [[16, 59]], [[16, 58]], [[16, 60]], [[16, 63]], [[16, 60]], [[16, 56]], [[16, 59]], [[16, 61]], [[16, 59]], [[16, 60]], [[16, 60]], [[16, 62]], [[16, 57]], [[16, 59]], [[16, 60]], [[16, 57]], [[16, 59]], [[16, 60]], [[16, 62]], [[16, 60]], [[16, 57]], [[16, 62]], [[16, 63]], [[16, 61]], [[16, 61]], [[16, 61]], [[16, 61]], [[16, 59]], [[16, 61]], [[16, 60]], [[16, 59]], [[16, 57]], [[16, 60]], [[16, 59]], [[16, 60]], [[16, 60]]]
>>> Packed: 16384 to 3818
[13, 1, 16, 256] 64 1 [[16, 63]]
INFO:root:Running MatMul
55490.0 ->  0.025108597285067875 ms

 === Layer 31 : layer3.2.conv1 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 196])
torch.Size([13, 16, 1024])
> step0: Inp | [13, 1, 16, 1024]
> step1: torch.Size([256, 1024]) tensor(0.1999)
> step2: torch.Size([16, 16, 1024])
> step3: Wgt | [[[16, 251]], [[16, 239]], [[16, 234]], [[16, 238]], [[16, 233]], [[16, 239]], [[16, 236]], [[16, 234]], [[16, 230]], [[16, 232]], [[16, 236]], [[16, 232]], [[16, 233]], [[16, 232]], [[16, 232]], [[16, 229]]]
>>> Packed: 16384 to 3760
[13, 1, 16, 1024] 16 1 [[16, 251]]
INFO:root:Running MatMul
50415.0 ->  0.022812217194570137 ms

 === Layer 32 : layer3.2.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) tensor(0.1980)
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 574]], [[16, 536]], [[16, 523]], [[16, 534]], [[16, 527]], [[16, 540]], [[16, 532]], [[16, 533]], [[16, 537]], [[16, 541]], [[16, 527]], [[16, 538]], [[16, 542]], [[16, 542]], [[16, 533]], [[16, 535]]]
>>> Packed: 36864 to 8594
[9, 3, 16, 1024] 16 1 [[16, 574]]
INFO:root:Running MatMul
78497 ->  0.03551900452488688 ms

 === Layer 33 : layer3.2.conv3 ===
torch.Size([256, 14, 14])
torch.Size([256, 196])
torch.Size([13, 16, 256])
> step0: Inp | [13, 1, 16, 256]
> step1: torch.Size([1024, 256]) tensor(0.1995)
> step2: torch.Size([64, 16, 256])
> step3: Wgt | [[[16, 69]], [[16, 59]], [[16, 62]], [[16, 57]], [[16, 60]], [[16, 59]], [[16, 59]], [[16, 61]], [[16, 57]], [[16, 59]], [[16, 58]], [[16, 59]], [[16, 57]], [[16, 61]], [[16, 63]], [[16, 60]], [[16, 59]], [[16, 57]], [[16, 59]], [[16, 63]], [[16, 56]], [[16, 60]], [[16, 60]], [[16, 61]], [[16, 60]], [[16, 59]], [[16, 60]], [[16, 57]], [[16, 62]], [[16, 57]], [[16, 58]], [[16, 57]], [[16, 60]], [[16, 60]], [[16, 59]], [[16, 60]], [[16, 60]], [[16, 60]], [[16, 58]], [[16, 61]], [[16, 60]], [[16, 56]], [[16, 59]], [[16, 61]], [[16, 60]], [[16, 57]], [[16, 59]], [[16, 60]], [[16, 58]], [[16, 59]], [[16, 59]], [[16, 59]], [[16, 61]], [[16, 61]], [[16, 57]], [[16, 57]], [[16, 62]], [[16, 58]], [[16, 64]], [[16, 60]], [[16, 61]], [[16, 63]], [[16, 60]], [[16, 67]]]
>>> Packed: 16384 to 3821
[13, 1, 16, 256] 64 1 [[16, 69]]
INFO:root:Running MatMul
55531.0 ->  0.02512714932126697 ms

 === Layer 34 : layer3.3.conv1 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 196])
torch.Size([13, 16, 1024])
> step0: Inp | [13, 1, 16, 1024]
> step1: torch.Size([256, 1024]) tensor(0.1990)
> step2: torch.Size([16, 16, 1024])
> step3: Wgt | [[[16, 246]], [[16, 244]], [[16, 238]], [[16, 236]], [[16, 236]], [[16, 235]], [[16, 234]], [[16, 233]], [[16, 239]], [[16, 236]], [[16, 239]], [[16, 238]], [[16, 237]], [[16, 239]], [[16, 239]], [[16, 238]]]
>>> Packed: 16384 to 3807
[13, 1, 16, 1024] 16 1 [[16, 246]]
INFO:root:Running MatMul
51025.0 ->  0.02308823529411765 ms

 === Layer 35 : layer3.3.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) tensor(0.1992)
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 567]], [[16, 542]], [[16, 543]], [[16, 531]], [[16, 533]], [[16, 539]], [[16, 532]], [[16, 537]], [[16, 531]], [[16, 536]], [[16, 536]], [[16, 535]], [[16, 531]], [[16, 534]], [[16, 543]], [[16, 545]]]
>>> Packed: 36864 to 8615
[9, 3, 16, 1024] 16 1 [[16, 567]]
INFO:root:Running MatMul
78684 ->  0.03560361990950226 ms

 === Layer 36 : layer3.3.conv3 ===
torch.Size([256, 14, 14])
torch.Size([256, 196])
torch.Size([13, 16, 256])
> step0: Inp | [13, 1, 16, 256]
> step1: torch.Size([1024, 256]) tensor(0.1989)
> step2: torch.Size([64, 16, 256])
> step3: Wgt | [[[16, 70]], [[16, 61]], [[16, 59]], [[16, 58]], [[16, 57]], [[16, 61]], [[16, 60]], [[16, 59]], [[16, 55]], [[16, 62]], [[16, 61]], [[16, 60]], [[16, 59]], [[16, 62]], [[16, 64]], [[16, 61]], [[16, 62]], [[16, 60]], [[16, 62]], [[16, 62]], [[16, 59]], [[16, 61]], [[16, 61]], [[16, 59]], [[16, 58]], [[16, 60]], [[16, 60]], [[16, 58]], [[16, 60]], [[16, 57]], [[16, 57]], [[16, 58]], [[16, 61]], [[16, 61]], [[16, 57]], [[16, 57]], [[16, 59]], [[16, 58]], [[16, 57]], [[16, 59]], [[16, 58]], [[16, 58]], [[16, 58]], [[16, 60]], [[16, 59]], [[16, 61]], [[16, 60]], [[16, 59]], [[16, 59]], [[16, 61]], [[16, 61]], [[16, 59]], [[16, 56]], [[16, 58]], [[16, 60]], [[16, 56]], [[16, 58]], [[16, 61]], [[16, 63]], [[16, 62]], [[16, 63]], [[16, 60]], [[16, 61]], [[16, 62]]]
>>> Packed: 16384 to 3825
[13, 1, 16, 256] 64 1 [[16, 70]]
INFO:root:Running MatMul
55583.0 ->  0.025150678733031676 ms

 === Layer 37 : layer3.4.conv1 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 196])
torch.Size([13, 16, 1024])
> step0: Inp | [13, 1, 16, 1024]
> step1: torch.Size([256, 1024]) tensor(0.1990)
> step2: torch.Size([16, 16, 1024])
> step3: Wgt | [[[16, 246]], [[16, 240]], [[16, 240]], [[16, 244]], [[16, 240]], [[16, 237]], [[16, 240]], [[16, 239]], [[16, 239]], [[16, 234]], [[16, 236]], [[16, 243]], [[16, 235]], [[16, 233]], [[16, 236]], [[16, 237]]]
>>> Packed: 16384 to 3819
[13, 1, 16, 1024] 16 1 [[16, 246]]
INFO:root:Running MatMul
51181.0 ->  0.023158823529411766 ms

 === Layer 38 : layer3.4.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) tensor(0.1996)
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 558]], [[16, 551]], [[16, 537]], [[16, 540]], [[16, 536]], [[16, 539]], [[16, 546]], [[16, 538]], [[16, 549]], [[16, 548]], [[16, 542]], [[16, 536]], [[16, 528]], [[16, 529]], [[16, 533]], [[16, 538]]]
>>> Packed: 36864 to 8648
[9, 3, 16, 1024] 16 1 [[16, 558]]
INFO:root:Running MatMul
78979 ->  0.03573710407239819 ms

 === Layer 39 : layer3.4.conv3 ===
torch.Size([256, 14, 14])
torch.Size([256, 196])
torch.Size([13, 16, 256])
> step0: Inp | [13, 1, 16, 256]
> step1: torch.Size([1024, 256]) tensor(0.2000)
> step2: torch.Size([64, 16, 256])
> step3: Wgt | [[[16, 67]], [[16, 61]], [[16, 59]], [[16, 62]], [[16, 58]], [[16, 57]], [[16, 60]], [[16, 58]], [[16, 57]], [[16, 62]], [[16, 59]], [[16, 59]], [[16, 60]], [[16, 61]], [[16, 62]], [[16, 60]], [[16, 59]], [[16, 62]], [[16, 60]], [[16, 62]], [[16, 63]], [[16, 60]], [[16, 59]], [[16, 58]], [[16, 59]], [[16, 60]], [[16, 63]], [[16, 59]], [[16, 57]], [[16, 53]], [[16, 57]], [[16, 54]], [[16, 61]], [[16, 58]], [[16, 60]], [[16, 58]], [[16, 60]], [[16, 58]], [[16, 58]], [[16, 59]], [[16, 61]], [[16, 61]], [[16, 59]], [[16, 60]], [[16, 61]], [[16, 60]], [[16, 60]], [[16, 58]], [[16, 58]], [[16, 61]], [[16, 58]], [[16, 58]], [[16, 57]], [[16, 61]], [[16, 61]], [[16, 58]], [[16, 58]], [[16, 60]], [[16, 61]], [[16, 62]], [[16, 65]], [[16, 60]], [[16, 62]], [[16, 61]]]
>>> Packed: 16384 to 3820
[13, 1, 16, 256] 64 1 [[16, 67]]
INFO:root:Running MatMul
55517.0 ->  0.02512081447963801 ms

 === Layer 40 : layer3.5.conv1 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 196])
torch.Size([13, 16, 1024])
> step0: Inp | [13, 1, 16, 1024]
> step1: torch.Size([256, 1024]) tensor(0.1995)
> step2: torch.Size([16, 16, 1024])
> step3: Wgt | [[[16, 249]], [[16, 246]], [[16, 236]], [[16, 238]], [[16, 242]], [[16, 242]], [[16, 237]], [[16, 237]], [[16, 238]], [[16, 238]], [[16, 236]], [[16, 243]], [[16, 241]], [[16, 229]], [[16, 235]], [[16, 240]]]
>>> Packed: 16384 to 3827
[13, 1, 16, 1024] 16 1 [[16, 249]]
INFO:root:Running MatMul
51286.0 ->  0.02320633484162896 ms

 === Layer 41 : layer3.5.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) tensor(0.1995)
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 559]], [[16, 595]], [[16, 537]], [[16, 542]], [[16, 530]], [[16, 538]], [[16, 538]], [[16, 544]], [[16, 538]], [[16, 532]], [[16, 537]], [[16, 536]], [[16, 544]], [[16, 536]], [[16, 530]], [[16, 538]]]
>>> Packed: 36864 to 8674
[9, 3, 16, 1024] 16 1 [[16, 559]]
INFO:root:Running MatMul
79213 ->  0.03584298642533937 ms

 === Layer 42 : layer3.5.conv3 ===
torch.Size([256, 14, 14])
torch.Size([256, 196])
torch.Size([13, 16, 256])
> step0: Inp | [13, 1, 16, 256]
> step1: torch.Size([1024, 256]) tensor(0.1985)
> step2: torch.Size([64, 16, 256])
> step3: Wgt | [[[16, 68]], [[16, 59]], [[16, 59]], [[16, 61]], [[16, 60]], [[16, 59]], [[16, 59]], [[16, 59]], [[16, 58]], [[16, 61]], [[16, 62]], [[16, 56]], [[16, 60]], [[16, 60]], [[16, 61]], [[16, 61]], [[16, 61]], [[16, 60]], [[16, 61]], [[16, 60]], [[16, 61]], [[16, 58]], [[16, 57]], [[16, 58]], [[16, 59]], [[16, 61]], [[16, 60]], [[16, 55]], [[16, 60]], [[16, 58]], [[16, 60]], [[16, 56]], [[16, 61]], [[16, 58]], [[16, 60]], [[16, 58]], [[16, 59]], [[16, 58]], [[16, 59]], [[16, 61]], [[16, 60]], [[16, 60]], [[16, 59]], [[16, 60]], [[16, 62]], [[16, 58]], [[16, 59]], [[16, 58]], [[16, 54]], [[16, 60]], [[16, 61]], [[16, 59]], [[16, 59]], [[16, 61]], [[16, 60]], [[16, 61]], [[16, 59]], [[16, 58]], [[16, 61]], [[16, 60]], [[16, 61]], [[16, 61]], [[16, 60]], [[16, 57]]]
>>> Packed: 16384 to 3810
[13, 1, 16, 256] 64 1 [[16, 68]]
INFO:root:Running MatMul
55387.0 ->  0.025061990950226244 ms

 === Layer 43 : layer4.0.conv1 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 196])
torch.Size([13, 16, 1024])
> step0: Inp | [13, 1, 16, 1024]
> step1: torch.Size([512, 1024]) tensor(0.1991)
> step2: torch.Size([32, 16, 1024])
> step3: Wgt | [[[16, 245]], [[16, 244]], [[16, 239]], [[16, 240]], [[16, 238]], [[16, 238]], [[16, 237]], [[16, 238]], [[16, 238]], [[16, 237]], [[16, 235]], [[16, 237]], [[16, 241]], [[16, 239]], [[16, 235]], [[16, 237]], [[16, 232]], [[16, 237]], [[16, 240]], [[16, 233]], [[16, 240]], [[16, 237]], [[16, 237]], [[16, 237]], [[16, 237]], [[16, 238]], [[16, 236]], [[16, 234]], [[16, 235]], [[16, 237]], [[16, 236]], [[16, 233]]]
>>> Packed: 32768 to 7597
[13, 1, 16, 1024] 32 1 [[16, 245]]
INFO:root:Running MatMul
101751.0 ->  0.04604117647058824 ms

 === Layer 44 : layer4.0.conv2 ===
torch.Size([512, 14, 14])
torch.Size([4608, 36])
torch.Size([3, 16, 4608])
> step0: Inp | [3, 5, 16, 1024]
> step1: torch.Size([512, 4608]) tensor(0.1997)
> step2: torch.Size([32, 16, 4608])
> step3: Wgt | [[[16, 1024], [16, 84]], [[16, 1024], [16, 71]], [[16, 1024], [16, 56]], [[16, 1024], [16, 42]], [[16, 1024], [16, 53]], [[16, 1024], [16, 51]], [[16, 1024], [16, 54]], [[16, 1024], [16, 60]], [[16, 1024], [16, 55]], [[16, 1024], [16, 49]], [[16, 1024], [16, 51]], [[16, 1024], [16, 54]], [[16, 1024], [16, 46]], [[16, 1024], [16, 49]], [[16, 1024], [16, 35]], [[16, 1024], [16, 48]], [[16, 1024], [16, 38]], [[16, 1024], [16, 43]], [[16, 1024], [16, 57]], [[16, 1024], [16, 56]], [[16, 1024], [16, 43]], [[16, 1024], [16, 46]], [[16, 1024], [16, 55]], [[16, 1024], [16, 56]], [[16, 1024], [16, 53]], [[16, 1024], [16, 56]], [[16, 1024], [16, 44]], [[16, 1024], [16, 57]], [[16, 1024], [16, 42]], [[16, 1024], [16, 45]], [[16, 1024], [16, 56]], [[16, 1024], [16, 51]]]
>>> Packed: 147456 to 32768
[3, 5, 16, 1024] 32 2 [[16, 1024], [16, 84]]
INFO:root:Running MatMul
123609 ->  0.0559316742081448 ms

 === Layer 45 : layer4.0.conv3 ===
torch.Size([512, 7, 7])
torch.Size([512, 49])
torch.Size([4, 16, 512])
> step0: Inp | [4, 1, 16, 512]
> step1: torch.Size([2048, 512]) tensor(0.1993)
> step2: torch.Size([128, 16, 512])
> step3: Wgt | [[[16, 128]], [[16, 123]], [[16, 124]], [[16, 120]], [[16, 123]], [[16, 118]], [[16, 122]], [[16, 118]], [[16, 119]], [[16, 123]], [[16, 119]], [[16, 118]], [[16, 119]], [[16, 120]], [[16, 120]], [[16, 121]], [[16, 119]], [[16, 117]], [[16, 119]], [[16, 118]], [[16, 118]], [[16, 120]], [[16, 116]], [[16, 119]], [[16, 119]], [[16, 119]], [[16, 120]], [[16, 121]], [[16, 120]], [[16, 123]], [[16, 117]], [[16, 118]], [[16, 122]], [[16, 121]], [[16, 122]], [[16, 123]], [[16, 118]], [[16, 120]], [[16, 119]], [[16, 122]], [[16, 121]], [[16, 117]], [[16, 121]], [[16, 121]], [[16, 114]], [[16, 117]], [[16, 117]], [[16, 121]], [[16, 119]], [[16, 121]], [[16, 118]], [[16, 116]], [[16, 118]], [[16, 120]], [[16, 121]], [[16, 119]], [[16, 120]], [[16, 120]], [[16, 120]], [[16, 122]], [[16, 122]], [[16, 119]], [[16, 121]], [[16, 120]], [[16, 118]], [[16, 117]], [[16, 122]], [[16, 120]], [[16, 120]], [[16, 121]], [[16, 121]], [[16, 121]], [[16, 116]], [[16, 118]], [[16, 119]], [[16, 120]], [[16, 117]], [[16, 119]], [[16, 122]], [[16, 120]], [[16, 113]], [[16, 117]], [[16, 119]], [[16, 118]], [[16, 118]], [[16, 117]], [[16, 120]], [[16, 122]], [[16, 122]], [[16, 121]], [[16, 120]], [[16, 122]], [[16, 123]], [[16, 119]], [[16, 120]], [[16, 119]], [[16, 117]], [[16, 118]], [[16, 118]], [[16, 119]], [[16, 121]], [[16, 121]], [[16, 123]], [[16, 120]], [[16, 118]], [[16, 119]], [[16, 120]], [[16, 119]], [[16, 119]], [[16, 122]], [[16, 121]], [[16, 120]], [[16, 119]], [[16, 119]], [[16, 117]], [[16, 118]], [[16, 119]], [[16, 120]], [[16, 121]], [[16, 120]], [[16, 120]], [[16, 116]], [[16, 120]], [[16, 116]], [[16, 118]], [[16, 116]], [[16, 120]], [[16, 118]]]
>>> Packed: 65536 to 15303
[4, 1, 16, 512] 128 1 [[16, 128]]
INFO:root:Running MatMul
64844.0 ->  0.02934117647058824 ms

 === Layer 46 : layer4.0.downsample.0 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 49])
torch.Size([4, 16, 1024])
> step0: Inp | [4, 1, 16, 1024]
> step1: torch.Size([2048, 1024]) tensor(0.1997)
> step2: torch.Size([128, 16, 1024])
> step3: Wgt | [[[16, 251]], [[16, 245]], [[16, 245]], [[16, 238]], [[16, 243]], [[16, 237]], [[16, 239]], [[16, 239]], [[16, 239]], [[16, 243]], [[16, 239]], [[16, 241]], [[16, 239]], [[16, 240]], [[16, 242]], [[16, 238]], [[16, 239]], [[16, 230]], [[16, 240]], [[16, 238]], [[16, 238]], [[16, 237]], [[16, 235]], [[16, 235]], [[16, 240]], [[16, 236]], [[16, 239]], [[16, 233]], [[16, 236]], [[16, 243]], [[16, 235]], [[16, 237]], [[16, 233]], [[16, 237]], [[16, 240]], [[16, 240]], [[16, 239]], [[16, 242]], [[16, 242]], [[16, 240]], [[16, 240]], [[16, 241]], [[16, 241]], [[16, 237]], [[16, 236]], [[16, 238]], [[16, 236]], [[16, 243]], [[16, 243]], [[16, 240]], [[16, 235]], [[16, 235]], [[16, 235]], [[16, 239]], [[16, 238]], [[16, 235]], [[16, 232]], [[16, 238]], [[16, 239]], [[16, 237]], [[16, 239]], [[16, 239]], [[16, 240]], [[16, 241]], [[16, 235]], [[16, 235]], [[16, 239]], [[16, 242]], [[16, 245]], [[16, 242]], [[16, 243]], [[16, 239]], [[16, 237]], [[16, 239]], [[16, 240]], [[16, 241]], [[16, 240]], [[16, 238]], [[16, 241]], [[16, 232]], [[16, 234]], [[16, 233]], [[16, 240]], [[16, 236]], [[16, 237]], [[16, 235]], [[16, 236]], [[16, 240]], [[16, 242]], [[16, 242]], [[16, 237]], [[16, 240]], [[16, 238]], [[16, 240]], [[16, 233]], [[16, 239]], [[16, 233]], [[16, 232]], [[16, 229]], [[16, 241]], [[16, 238]], [[16, 235]], [[16, 239]], [[16, 240]], [[16, 241]], [[16, 240]], [[16, 238]], [[16, 234]], [[16, 239]], [[16, 241]], [[16, 234]], [[16, 241]], [[16, 240]], [[16, 238]], [[16, 232]], [[16, 243]], [[16, 236]], [[16, 242]], [[16, 238]], [[16, 238]], [[16, 235]], [[16, 238]], [[16, 232]], [[16, 227]], [[16, 238]], [[16, 240]], [[16, 239]], [[16, 241]]]
>>> Packed: 131072 to 30501
[4, 1, 16, 1024] 128 1 [[16, 251]]
INFO:root:Running MatMul
125667.0 ->  0.05686289592760181 ms

 === Layer 47 : layer4.1.conv1 ===
torch.Size([2048, 7, 7])
torch.Size([2048, 49])
torch.Size([4, 16, 2048])
> step0: Inp | [4, 2, 16, 1024]
> step1: torch.Size([512, 2048]) tensor(0.1996)
> step2: torch.Size([32, 16, 2048])
> step3: Wgt | [[[16, 494]], [[16, 483]], [[16, 485]], [[16, 475]], [[16, 476]], [[16, 472]], [[16, 475]], [[16, 474]], [[16, 479]], [[16, 471]], [[16, 476]], [[16, 472]], [[16, 474]], [[16, 464]], [[16, 472]], [[16, 469]], [[16, 471]], [[16, 483]], [[16, 469]], [[16, 476]], [[16, 476]], [[16, 471]], [[16, 476]], [[16, 476]], [[16, 478]], [[16, 478]], [[16, 471]], [[16, 481]], [[16, 470]], [[16, 479]], [[16, 474]], [[16, 468]]]
>>> Packed: 65536 to 15208
[4, 2, 16, 1024] 32 1 [[16, 494]]
INFO:root:Running MatMul
61851 ->  0.0279868778280543 ms

 === Layer 48 : layer4.1.conv2 ===
torch.Size([512, 7, 7])
torch.Size([4608, 25])
torch.Size([2, 16, 4608])
> step0: Inp | [2, 5, 16, 1024]
> step1: torch.Size([512, 4608]) tensor(0.1979)
> step2: torch.Size([32, 16, 4608])
> step3: Wgt | [[[16, 1024], [16, 90]], [[16, 1024], [16, 71]], [[16, 1024], [16, 59]], [[16, 1024], [16, 69]], [[16, 1024], [16, 64]], [[16, 1024], [16, 55]], [[16, 1024], [16, 42]], [[16, 1024], [16, 49]], [[16, 1024], [16, 51]], [[16, 1024], [16, 50]], [[16, 1024], [16, 45]], [[16, 1024], [16, 47]], [[16, 1024], [16, 61]], [[16, 1024], [16, 54]], [[16, 1024], [16, 48]], [[16, 1024], [16, 55]], [[16, 1024], [16, 50]], [[16, 1024], [16, 36]], [[16, 1024], [16, 55]], [[16, 1024], [16, 57]], [[16, 1024], [16, 47]], [[16, 1024], [16, 37]], [[16, 1024], [16, 26]], [[16, 1024], [16, 49]], [[16, 1024], [16, 34]], [[16, 1024], [16, 32]], [[16, 1024], [16, 42]], [[16, 1024], [16, 46]], [[16, 1024], [16, 43]], [[16, 1024], [16, 58]], [[16, 1024], [16, 42]], [[16, 1024], [16, 49]]]
>>> Packed: 147456 to 32768
[2, 5, 16, 1024] 32 2 [[16, 1024], [16, 90]]
INFO:root:Running MatMul
82423 ->  0.03729547511312217 ms

 === Layer 49 : layer4.1.conv3 ===
torch.Size([512, 7, 7])
torch.Size([512, 49])
torch.Size([4, 16, 512])
> step0: Inp | [4, 1, 16, 512]
> step1: torch.Size([2048, 512]) tensor(0.1993)
> step2: torch.Size([128, 16, 512])
> step3: Wgt | [[[16, 128]], [[16, 121]], [[16, 120]], [[16, 120]], [[16, 121]], [[16, 123]], [[16, 117]], [[16, 119]], [[16, 121]], [[16, 120]], [[16, 120]], [[16, 119]], [[16, 118]], [[16, 118]], [[16, 122]], [[16, 121]], [[16, 118]], [[16, 119]], [[16, 119]], [[16, 118]], [[16, 118]], [[16, 120]], [[16, 116]], [[16, 119]], [[16, 120]], [[16, 119]], [[16, 121]], [[16, 119]], [[16, 119]], [[16, 120]], [[16, 119]], [[16, 117]], [[16, 120]], [[16, 117]], [[16, 117]], [[16, 121]], [[16, 119]], [[16, 120]], [[16, 121]], [[16, 121]], [[16, 118]], [[16, 120]], [[16, 120]], [[16, 123]], [[16, 120]], [[16, 117]], [[16, 116]], [[16, 120]], [[16, 120]], [[16, 121]], [[16, 117]], [[16, 120]], [[16, 119]], [[16, 118]], [[16, 119]], [[16, 118]], [[16, 119]], [[16, 121]], [[16, 120]], [[16, 119]], [[16, 119]], [[16, 116]], [[16, 119]], [[16, 118]], [[16, 117]], [[16, 120]], [[16, 121]], [[16, 119]], [[16, 122]], [[16, 119]], [[16, 120]], [[16, 119]], [[16, 119]], [[16, 118]], [[16, 121]], [[16, 121]], [[16, 121]], [[16, 118]], [[16, 121]], [[16, 119]], [[16, 118]], [[16, 119]], [[16, 119]], [[16, 119]], [[16, 118]], [[16, 120]], [[16, 118]], [[16, 122]], [[16, 122]], [[16, 124]], [[16, 121]], [[16, 123]], [[16, 117]], [[16, 120]], [[16, 116]], [[16, 118]], [[16, 118]], [[16, 115]], [[16, 122]], [[16, 119]], [[16, 124]], [[16, 122]], [[16, 118]], [[16, 119]], [[16, 120]], [[16, 120]], [[16, 120]], [[16, 118]], [[16, 118]], [[16, 118]], [[16, 119]], [[16, 115]], [[16, 117]], [[16, 118]], [[16, 118]], [[16, 121]], [[16, 117]], [[16, 119]], [[16, 119]], [[16, 115]], [[16, 118]], [[16, 117]], [[16, 121]], [[16, 117]], [[16, 119]], [[16, 118]], [[16, 115]], [[16, 120]]]
>>> Packed: 65536 to 15265
[4, 1, 16, 512] 128 1 [[16, 128]]
INFO:root:Running MatMul
64692.0 ->  0.029272398190045252 ms

 === Layer 50 : layer4.2.conv1 ===
torch.Size([2048, 7, 7])
torch.Size([2048, 49])
torch.Size([4, 16, 2048])
> step0: Inp | [4, 2, 16, 1024]
> step1: torch.Size([512, 2048]) tensor(0.1992)
> step2: torch.Size([32, 16, 2048])
> step3: Wgt | [[[16, 486]], [[16, 486]], [[16, 476]], [[16, 467]], [[16, 478]], [[16, 472]], [[16, 472]], [[16, 470]], [[16, 470]], [[16, 471]], [[16, 471]], [[16, 469]], [[16, 481]], [[16, 472]], [[16, 470]], [[16, 476]], [[16, 473]], [[16, 477]], [[16, 474]], [[16, 472]], [[16, 469]], [[16, 477]], [[16, 461]], [[16, 483]], [[16, 477]], [[16, 469]], [[16, 466]], [[16, 476]], [[16, 474]], [[16, 473]], [[16, 474]], [[16, 472]]]
>>> Packed: 65536 to 15154
[4, 2, 16, 1024] 32 1 [[16, 486]]
INFO:root:Running MatMul
61633 ->  0.027888235294117648 ms

 === Layer 51 : layer4.2.conv2 ===
torch.Size([512, 7, 7])
torch.Size([4608, 25])
torch.Size([2, 16, 4608])
> step0: Inp | [2, 5, 16, 1024]
> step1: torch.Size([512, 4608]) tensor(0.1984)
> step2: torch.Size([32, 16, 4608])
> step3: Wgt | [[[16, 1024], [16, 88]], [[16, 1024], [16, 64]], [[16, 1024], [16, 43]], [[16, 1024], [16, 40]], [[16, 1024], [16, 60]], [[16, 1024], [16, 40]], [[16, 1024], [16, 32]], [[16, 1024], [16, 49]], [[16, 1024], [16, 27]], [[16, 1024], [16, 40]], [[16, 1024], [16, 25]], [[16, 1024], [16, 49]], [[16, 1024], [16, 40]], [[16, 1024], [16, 51]], [[16, 1024], [16, 40]], [[16, 1024], [16, 47]], [[16, 1024], [16, 44]], [[16, 1024], [16, 35]], [[16, 1024], [16, 50]], [[16, 1024], [16, 36]], [[16, 1024], [16, 39]], [[16, 1024], [16, 47]], [[16, 1024], [16, 46]], [[16, 1024], [16, 40]], [[16, 1024], [16, 51]], [[16, 1024], [16, 50]], [[16, 1024], [16, 33]], [[16, 1024], [16, 42]], [[16, 1024], [16, 44]], [[16, 1024], [16, 40]], [[16, 1024], [16, 54]], [[16, 1024], [16, 50]]]
>>> Packed: 147456 to 32768
[2, 5, 16, 1024] 32 2 [[16, 1024], [16, 88]]
INFO:root:Running MatMul
82424 ->  0.037295927601809956 ms

 === Layer 52 : layer4.2.conv3 ===
torch.Size([512, 7, 7])
torch.Size([512, 49])
torch.Size([4, 16, 512])
> step0: Inp | [4, 1, 16, 512]
> step1: torch.Size([2048, 512]) tensor(0.1990)
> step2: torch.Size([128, 16, 512])
> step3: Wgt | [[[16, 130]], [[16, 121]], [[16, 117]], [[16, 121]], [[16, 122]], [[16, 122]], [[16, 119]], [[16, 116]], [[16, 123]], [[16, 119]], [[16, 117]], [[16, 118]], [[16, 120]], [[16, 119]], [[16, 117]], [[16, 119]], [[16, 120]], [[16, 118]], [[16, 121]], [[16, 116]], [[16, 118]], [[16, 119]], [[16, 116]], [[16, 120]], [[16, 119]], [[16, 121]], [[16, 120]], [[16, 113]], [[16, 117]], [[16, 121]], [[16, 114]], [[16, 118]], [[16, 123]], [[16, 122]], [[16, 120]], [[16, 122]], [[16, 119]], [[16, 120]], [[16, 121]], [[16, 122]], [[16, 118]], [[16, 119]], [[16, 122]], [[16, 118]], [[16, 115]], [[16, 117]], [[16, 117]], [[16, 120]], [[16, 120]], [[16, 118]], [[16, 117]], [[16, 113]], [[16, 120]], [[16, 120]], [[16, 118]], [[16, 123]], [[16, 120]], [[16, 121]], [[16, 120]], [[16, 121]], [[16, 123]], [[16, 117]], [[16, 117]], [[16, 121]], [[16, 119]], [[16, 119]], [[16, 120]], [[16, 118]], [[16, 122]], [[16, 123]], [[16, 123]], [[16, 122]], [[16, 119]], [[16, 116]], [[16, 121]], [[16, 122]], [[16, 121]], [[16, 123]], [[16, 121]], [[16, 117]], [[16, 116]], [[16, 119]], [[16, 120]], [[16, 121]], [[16, 118]], [[16, 117]], [[16, 119]], [[16, 124]], [[16, 124]], [[16, 121]], [[16, 119]], [[16, 122]], [[16, 120]], [[16, 122]], [[16, 115]], [[16, 119]], [[16, 117]], [[16, 119]], [[16, 118]], [[16, 116]], [[16, 124]], [[16, 122]], [[16, 120]], [[16, 120]], [[16, 120]], [[16, 119]], [[16, 119]], [[16, 118]], [[16, 117]], [[16, 118]], [[16, 123]], [[16, 119]], [[16, 119]], [[16, 116]], [[16, 118]], [[16, 123]], [[16, 118]], [[16, 121]], [[16, 120]], [[16, 120]], [[16, 116]], [[16, 113]], [[16, 113]], [[16, 113]], [[16, 118]], [[16, 116]], [[16, 116]], [[16, 120]]]
>>> Packed: 65536 to 15266
[4, 1, 16, 512] 128 1 [[16, 130]]
INFO:root:Running MatMul
64697.0 ->  0.029274660633484165 ms

 === Layer 53 : fc ===
torch.Size([2048])
torch.Size([2048, 1])
torch.Size([1, 16, 2048])
> step0: Inp | [1, 2, 16, 1024]
> step1: torch.Size([1000, 2048]) tensor(0.1995)
> step2: torch.Size([63, 16, 2048])
> step3: Wgt | [[[16, 448]], [[16, 427]], [[16, 398]], [[16, 414]], [[16, 411]], [[16, 410]], [[16, 451]], [[16, 428]], [[16, 392]], [[16, 444]], [[16, 404]], [[16, 419]], [[16, 425]], [[16, 407]], [[16, 421]], [[16, 420]], [[16, 418]], [[16, 418]], [[16, 417]], [[16, 417]], [[16, 437]], [[16, 421]], [[16, 434]], [[16, 401]], [[16, 461]], [[16, 475]], [[16, 477]], [[16, 473]], [[16, 475]], [[16, 478]], [[16, 474]], [[16, 477]], [[16, 469]], [[16, 481]], [[16, 474]], [[16, 477]], [[16, 471]], [[16, 474]], [[16, 474]], [[16, 469]], [[16, 480]], [[16, 465]], [[16, 478]], [[16, 478]], [[16, 476]], [[16, 481]], [[16, 480]], [[16, 475]], [[16, 473]], [[16, 478]], [[16, 478]], [[16, 477]], [[16, 471]], [[16, 484]], [[16, 468]], [[16, 479]], [[16, 479]], [[16, 456]], [[16, 411]], [[16, 406]], [[16, 457]], [[16, 437]], [[16, 1024], [16, 852]]]
>>> Packed: 129024 to 28952
[1, 2, 16, 1024] 63 1 [[16, 448]]
INFO:root:Running MatMul
29504 ->  0.013350226244343892 ms
The overall latecy is: 1.7912945701357468
INFO:root:####################Finish######################


conv1 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
layer1.0.conv1 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer1.0.conv2 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer1.1.conv1 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer1.1.conv2 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer2.0.conv1 Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
layer2.0.conv2 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer2.0.downsample.0 Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
layer2.1.conv1 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer2.1.conv2 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer3.0.conv1 Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
layer3.0.conv2 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer3.0.downsample.0 Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
layer3.1.conv1 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer3.1.conv2 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer4.0.conv1 Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
layer4.0.conv2 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer4.0.downsample.0 Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
layer4.1.conv1 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
layer4.1.conv2 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
fc Linear(in_features=512, out_features=1000, bias=True)
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
Sequential(
  (0): BasicBlock(
    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (1): BasicBlock(
    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
BasicBlock(
  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
BasicBlock(
  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Sequential(
  (0): BasicBlock(
    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (downsample): Sequential(
      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
BasicBlock(
  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (downsample): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
Sequential(
  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
BasicBlock(
  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Sequential(
  (0): BasicBlock(
    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (downsample): Sequential(
      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
BasicBlock(
  (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (downsample): Sequential(
    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
Sequential(
  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
BasicBlock(
  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Sequential(
  (0): BasicBlock(
    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (downsample): Sequential(
      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1): BasicBlock(
    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
BasicBlock(
  (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (downsample): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
Sequential(
  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
BasicBlock(
  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Linear(in_features=512, out_features=1000, bias=True)
21 21 21 21 21
21

 === Layer 0 : conv1 ===
torch.Size([3, 224, 224])
torch.Size([147, 11881])
torch.Size([743, 16, 147])
> step0: Inp | [743, 1, 16, 147]
> step1: torch.Size([64, 147]) tensor(0.3998)
> step2: torch.Size([4, 16, 147])
> step3: Wgt | [[[16, 70]], [[16, 60]], [[16, 65]], [[16, 62]]]
>>> Packed: 588 to 257
[743, 1, 16, 147] 4 1 [[16, 70]]
212583.0 ->  0.09619140271493214 ms

 === Layer 1 : layer1.0.conv1 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) tensor(0.3991)
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 264]], [[16, 223]], [[16, 235]], [[16, 225]]]
>>> Packed: 2304 to 947
[183, 1, 16, 576] 4 1 [[16, 264]]
185444.0 ->  0.08391131221719457 ms

 === Layer 2 : layer1.0.conv2 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) tensor(0.3987)
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 268]], [[16, 221]], [[16, 227]], [[16, 240]]]
>>> Packed: 2304 to 956
[183, 1, 16, 576] 4 1 [[16, 268]]
185276.0 ->  0.08383529411764706 ms

 === Layer 3 : layer1.1.conv1 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) tensor(0.3996)
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 258]], [[16, 224]], [[16, 248]], [[16, 226]]]
>>> Packed: 2304 to 956
[183, 1, 16, 576] 4 1 [[16, 258]]
188007.0 ->  0.0850710407239819 ms

 === Layer 4 : layer1.1.conv2 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) tensor(0.3990)
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 255]], [[16, 229]], [[16, 229]], [[16, 235]]]
>>> Packed: 2304 to 948
[183, 1, 16, 576] 4 1 [[16, 255]]
181245.0 ->  0.08201131221719457 ms

 === Layer 5 : layer2.0.conv1 ===
torch.Size([64, 56, 56])
torch.Size([576, 729])
torch.Size([46, 16, 576])
> step0: Inp | [46, 1, 16, 576]
> step1: torch.Size([128, 576]) tensor(0.4000)
> step2: torch.Size([8, 16, 576])
> step3: Wgt | [[[16, 251]], [[16, 238]], [[16, 234]], [[16, 228]], [[16, 237]], [[16, 237]], [[16, 240]], [[16, 235]]]
>>> Packed: 4608 to 1900
[46, 1, 16, 576] 8 1 [[16, 251]]
90740.0 ->  0.041058823529411766 ms

 === Layer 6 : layer2.0.conv2 ===
torch.Size([128, 28, 28])
torch.Size([1152, 676])
torch.Size([43, 16, 1152])
> step0: Inp | [43, 2, 16, 1024]
> step1: torch.Size([128, 1152]) tensor(0.3986)
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 471]], [[16, 487]], [[16, 482]], [[16, 470]], [[16, 464]], [[16, 455]], [[16, 481]], [[16, 455]]]
>>> Packed: 9216 to 3765
[43, 2, 16, 1024] 8 1 [[16, 471]]
166355.0 ->  0.0752737556561086 ms

 === Layer 7 : layer2.0.downsample.0 ===
torch.Size([64, 56, 56])
torch.Size([64, 784])
torch.Size([49, 16, 64])
> step0: Inp | [49, 1, 16, 64]
> step1: torch.Size([128, 64]) tensor(0.3997)
> step2: torch.Size([8, 16, 64])
> step3: Wgt | [[[16, 35]], [[16, 25]], [[16, 27]], [[16, 26]], [[16, 27]], [[16, 28]], [[16, 26]], [[16, 26]]]
>>> Packed: 512 to 220
[49, 1, 16, 64] 8 1 [[16, 35]]
13671.0 ->  0.006185972850678734 ms

 === Layer 8 : layer2.1.conv1 ===
torch.Size([128, 28, 28])
torch.Size([1152, 676])
torch.Size([43, 16, 1152])
> step0: Inp | [43, 2, 16, 1024]
> step1: torch.Size([128, 1152]) tensor(0.3992)
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 478]], [[16, 458]], [[16, 468]], [[16, 474]], [[16, 472]], [[16, 480]], [[16, 466]], [[16, 461]]]
>>> Packed: 9216 to 3757
[43, 2, 16, 1024] 8 1 [[16, 478]]
165028.0 ->  0.07467330316742082 ms

 === Layer 9 : layer2.1.conv2 ===
torch.Size([128, 28, 28])
torch.Size([1152, 676])
torch.Size([43, 16, 1152])
> step0: Inp | [43, 2, 16, 1024]
> step1: torch.Size([128, 1152]) tensor(0.3986)
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 480]], [[16, 478]], [[16, 467]], [[16, 456]], [[16, 464]], [[16, 495]], [[16, 452]], [[16, 493]]]
>>> Packed: 9216 to 3785
[43, 2, 16, 1024] 8 1 [[16, 480]]
168179.0 ->  0.07609909502262444 ms

 === Layer 10 : layer3.0.conv1 ===
torch.Size([128, 28, 28])
torch.Size([1152, 169])
torch.Size([11, 16, 1152])
> step0: Inp | [11, 2, 16, 1024]
> step1: torch.Size([256, 1152]) tensor(0.3990)
> step2: torch.Size([16, 16, 1152])
> step3: Wgt | [[[16, 509]], [[16, 462]], [[16, 498]], [[16, 444]], [[16, 475]], [[16, 474]], [[16, 466]], [[16, 470]], [[16, 484]], [[16, 468]], [[16, 473]], [[16, 456]], [[16, 463]], [[16, 470]], [[16, 455]], [[16, 476]]]
>>> Packed: 18432 to 7543
[11, 2, 16, 1024] 16 1 [[16, 509]]
85787.0 ->  0.03881764705882353 ms

 === Layer 11 : layer3.0.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) tensor(0.3985)
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 932]], [[16, 933]], [[16, 949]], [[16, 913]], [[16, 933]], [[16, 975]], [[16, 929]], [[16, 892]], [[16, 968]], [[16, 914]], [[16, 917]], [[16, 914]], [[16, 929]], [[16, 947]], [[16, 913]], [[16, 948]]]
>>> Packed: 36864 to 14906
[9, 3, 16, 1024] 16 1 [[16, 932]]
137650.0 ->  0.06228506787330317 ms

 === Layer 12 : layer3.0.downsample.0 ===
torch.Size([128, 28, 28])
torch.Size([128, 196])
torch.Size([13, 16, 128])
> step0: Inp | [13, 1, 16, 128]
> step1: torch.Size([256, 128]) tensor(0.3987)
> step2: torch.Size([16, 16, 128])
> step3: Wgt | [[[16, 58]], [[16, 51]], [[16, 56]], [[16, 52]], [[16, 53]], [[16, 55]], [[16, 57]], [[16, 50]], [[16, 54]], [[16, 56]], [[16, 46]], [[16, 51]], [[16, 50]], [[16, 58]], [[16, 51]], [[16, 53]]]
>>> Packed: 2048 to 851
[13, 1, 16, 128] 16 1 [[16, 58]]
12606.0 ->  0.005704072398190045 ms

 === Layer 13 : layer3.1.conv1 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) tensor(0.3993)
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 998]], [[16, 899]], [[16, 925]], [[16, 925]], [[16, 918]], [[16, 910]], [[16, 947]], [[16, 933]], [[16, 924]], [[16, 930]], [[16, 958]], [[16, 915]], [[16, 939]], [[16, 1024], [16, 115]], [[16, 802]], [[16, 944]]]
>>> Packed: 36864 to 14891
[9, 3, 16, 1024] 16 1 [[16, 998]]
139100.0 ->  0.06294117647058824 ms

 === Layer 14 : layer3.1.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) tensor(0.3990)
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 999]], [[16, 919]], [[16, 952]], [[16, 977]], [[16, 899]], [[16, 973]], [[16, 912]], [[16, 900]], [[16, 915]], [[16, 890]], [[16, 946]], [[16, 975]], [[16, 931]], [[16, 928]], [[16, 899]], [[16, 956]]]
>>> Packed: 36864 to 14971
[9, 3, 16, 1024] 16 1 [[16, 999]]
139193.0 ->  0.06298325791855204 ms

 === Layer 15 : layer4.0.conv1 ===
torch.Size([256, 14, 14])
torch.Size([2304, 36])
torch.Size([3, 16, 2304])
> step0: Inp | [3, 3, 16, 1024]
> step1: torch.Size([512, 2304]) tensor(0.3993)
> step2: torch.Size([32, 16, 2304])
> step3: Wgt | [[[16, 994]], [[16, 926]], [[16, 923]], [[16, 908]], [[16, 946]], [[16, 940]], [[16, 937]], [[16, 930]], [[16, 926]], [[16, 917]], [[16, 920]], [[16, 925]], [[16, 933]], [[16, 926]], [[16, 945]], [[16, 911]], [[16, 955]], [[16, 956]], [[16, 932]], [[16, 914]], [[16, 933]], [[16, 925]], [[16, 914]], [[16, 939]], [[16, 940]], [[16, 926]], [[16, 935]], [[16, 935]], [[16, 939]], [[16, 935]], [[16, 1008]], [[16, 903]]]
>>> Packed: 73728 to 29896
[3, 3, 16, 1024] 32 1 [[16, 994]]
92058.0 ->  0.04165520361990951 ms

 === Layer 16 : layer4.0.conv2 ===
torch.Size([512, 7, 7])
torch.Size([4608, 25])
torch.Size([2, 16, 4608])
> step0: Inp | [2, 5, 16, 1024]
> step1: torch.Size([512, 4608]) tensor(0.3997)
> step2: torch.Size([32, 16, 4608])
> step3: Wgt | [[[16, 1024], [16, 899]], [[16, 1024], [16, 815]], [[16, 1024], [16, 833]], [[16, 1024], [16, 855]], [[16, 1024], [16, 812]], [[16, 1024], [16, 843]], [[16, 1024], [16, 830]], [[16, 1024], [16, 840]], [[16, 1024], [16, 861]], [[16, 1024], [16, 838]], [[16, 1024], [16, 844]], [[16, 1024], [16, 812]], [[16, 1024], [16, 857]], [[16, 1024], [16, 837]], [[16, 1024], [16, 852]], [[16, 1024], [16, 893]], [[16, 1024], [16, 851]], [[16, 1024], [16, 833]], [[16, 1024], [16, 838]], [[16, 1024], [16, 855]], [[16, 1024], [16, 832]], [[16, 1024], [16, 862]], [[16, 1024], [16, 857]], [[16, 1024], [16, 869]], [[16, 1024], [16, 825]], [[16, 1024], [16, 833]], [[16, 1024], [16, 849]], [[16, 1024], [16, 826]], [[16, 1024], [16, 839]], [[16, 1024], [16, 868]], [[16, 1024], [16, 783]], [[16, 1024], [16, 851]]]
>>> Packed: 147456 to 32768
[2, 5, 16, 1024] 32 2 [[16, 1024], [16, 899]]
132377.0 ->  0.059899095022624436 ms

 === Layer 17 : layer4.0.downsample.0 ===
torch.Size([256, 14, 14])
torch.Size([256, 49])
torch.Size([4, 16, 256])
> step0: Inp | [4, 1, 16, 256]
> step1: torch.Size([512, 256]) tensor(0.3993)
> step2: torch.Size([32, 16, 256])
> step3: Wgt | [[[16, 112]], [[16, 105]], [[16, 111]], [[16, 108]], [[16, 101]], [[16, 111]], [[16, 108]], [[16, 105]], [[16, 100]], [[16, 105]], [[16, 101]], [[16, 106]], [[16, 106]], [[16, 104]], [[16, 107]], [[16, 105]], [[16, 101]], [[16, 105]], [[16, 104]], [[16, 111]], [[16, 100]], [[16, 100]], [[16, 106]], [[16, 111]], [[16, 107]], [[16, 104]], [[16, 110]], [[16, 106]], [[16, 102]], [[16, 103]], [[16, 101]], [[16, 102]]]
>>> Packed: 8192 to 3368
[4, 1, 16, 256] 32 1 [[16, 112]]
14517.0 ->  0.006568778280542987 ms

 === Layer 18 : layer4.1.conv1 ===
torch.Size([512, 7, 7])
torch.Size([4608, 25])
torch.Size([2, 16, 4608])
> step0: Inp | [2, 5, 16, 1024]
> step1: torch.Size([512, 4608]) tensor(0.3979)
> step2: torch.Size([32, 16, 4608])
> step3: Wgt | [[[16, 1024], [16, 916]], [[16, 1024], [16, 831]], [[16, 1024], [16, 838]], [[16, 1024], [16, 849]], [[16, 1024], [16, 815]], [[16, 1024], [16, 799]], [[16, 1024], [16, 828]], [[16, 1024], [16, 962]], [[16, 1024], [16, 771]], [[16, 1024], [16, 804]], [[16, 1024], [16, 858]], [[16, 1024], [16, 835]], [[16, 1024], [16, 886]], [[16, 1024], [16, 808]], [[16, 1024], [16, 794]], [[16, 1024], [16, 816]], [[16, 1024], [16, 860]], [[16, 1024], [16, 838]], [[16, 1024], [16, 815]], [[16, 1024], [16, 804]], [[16, 1024], [16, 827]], [[16, 1024], [16, 861]], [[16, 1024], [16, 809]], [[16, 1024], [16, 844]], [[16, 1024], [16, 779]], [[16, 1024], [16, 950]], [[16, 1024], [16, 800]], [[16, 1024], [16, 858]], [[16, 1024], [16, 822]], [[16, 1024], [16, 811]], [[16, 1024], [16, 843]], [[16, 1024], [16, 816]]]
>>> Packed: 147456 to 32768
[2, 5, 16, 1024] 32 2 [[16, 1024], [16, 916]]
132342.0 ->  0.05988325791855204 ms

 === Layer 19 : layer4.1.conv2 ===
torch.Size([512, 7, 7])
torch.Size([4608, 25])
torch.Size([2, 16, 4608])
> step0: Inp | [2, 5, 16, 1024]
> step1: torch.Size([512, 4608]) tensor(0.3977)
> step2: torch.Size([32, 16, 4608])
> step3: Wgt | [[[16, 1024], [16, 928]], [[16, 1024], [16, 829]], [[16, 1024], [16, 817]], [[16, 1024], [16, 820]], [[16, 1024], [16, 810]], [[16, 1024], [16, 873]], [[16, 1024], [16, 812]], [[16, 1024], [16, 835]], [[16, 1024], [16, 840]], [[16, 1024], [16, 849]], [[16, 1024], [16, 826]], [[16, 1024], [16, 817]], [[16, 1024], [16, 855]], [[16, 1024], [16, 821]], [[16, 1024], [16, 822]], [[16, 1024], [16, 830]], [[16, 1024], [16, 863]], [[16, 1024], [16, 824]], [[16, 1024], [16, 808]], [[16, 1024], [16, 843]], [[16, 1024], [16, 832]], [[16, 1024], [16, 813]], [[16, 1024], [16, 850]], [[16, 1024], [16, 841]], [[16, 1024], [16, 845]], [[16, 1024], [16, 830]], [[16, 1024], [16, 812]], [[16, 1024], [16, 862]], [[16, 1024], [16, 809]], [[16, 1024], [16, 816]], [[16, 1024], [16, 817]], [[16, 1024], [16, 850]]]
>>> Packed: 147456 to 32768
[2, 5, 16, 1024] 32 2 [[16, 1024], [16, 928]]
132376.0 ->  0.059898642533936654 ms

 === Layer 20 : fc ===
torch.Size([512])
torch.Size([512, 1])
torch.Size([1, 16, 512])
> step0: Inp | [1, 1, 16, 512]
> step1: torch.Size([1000, 512]) tensor(0.4000)
> step2: torch.Size([63, 16, 512])
> step3: Wgt | [[[16, 217]], [[16, 201]], [[16, 197]], [[16, 203]], [[16, 196]], [[16, 197]], [[16, 206]], [[16, 208]], [[16, 193]], [[16, 206]], [[16, 200]], [[16, 206]], [[16, 205]], [[16, 209]], [[16, 202]], [[16, 208]], [[16, 200]], [[16, 203]], [[16, 191]], [[16, 207]], [[16, 189]], [[16, 207]], [[16, 201]], [[16, 204]], [[16, 207]], [[16, 207]], [[16, 216]], [[16, 214]], [[16, 220]], [[16, 228]], [[16, 212]], [[16, 218]], [[16, 220]], [[16, 212]], [[16, 216]], [[16, 206]], [[16, 218]], [[16, 217]], [[16, 218]], [[16, 217]], [[16, 208]], [[16, 213]], [[16, 214]], [[16, 219]], [[16, 212]], [[16, 223]], [[16, 218]], [[16, 215]], [[16, 220]], [[16, 228]], [[16, 209]], [[16, 218]], [[16, 217]], [[16, 223]], [[16, 215]], [[16, 214]], [[16, 227]], [[16, 207]], [[16, 212]], [[16, 196]], [[16, 217]], [[16, 196]], [[16, 487]]]
>>> Packed: 32256 to 13510
[1, 1, 16, 512] 63 1 [[16, 217]]
14531.0 ->  0.006575113122171946 ms
The overall latecy is: 1.1715226244343893

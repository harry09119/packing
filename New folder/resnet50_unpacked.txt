 Resnet50 Dense base SA muxsize 4
 
 === Layer 0 : conv1 ===
torch.Size([3, 224, 224])
torch.Size([147, 11881])
torch.Size([743, 16, 147])
> step0: Inp | [743, 1, 16, 147]
> step1: torch.Size([64, 147]) 1
> step2: torch.Size([4, 16, 147])
> step3: Wgt | [[[16, 147]], [[16, 147]], [[16, 147]], [[16, 147]]]
>>> Packed: 588 to 588
[743, 1, 16, 147] 4 1 [[16, 147]]
INFO:root:Running MatMul
457851.0 ->  0.20717239819004526 ms

 === Layer 1 : layer1.0.conv1 ===
torch.Size([64, 56, 56])
torch.Size([64, 3136])
torch.Size([196, 16, 64])
> step0: Inp | [196, 1, 16, 64]
> step1: torch.Size([64, 64]) 1
> step2: torch.Size([4, 16, 64])
> step3: Wgt | [[[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]]]
>>> Packed: 256 to 256
[196, 1, 16, 64] 4 1 [[16, 64]]
INFO:root:Running MatMul
55744.0 ->  0.025223529411764707 ms

 === Layer 2 : layer1.0.conv2 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) 1
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 576]], [[16, 576]], [[16, 576]], [[16, 576]]]
>>> Packed: 2304 to 2304
[183, 1, 16, 576] 4 1 [[16, 576]]
INFO:root:Running MatMul
427348.0 ->  0.19337013574660636 ms

 === Layer 3 : layer1.0.conv3 ===
torch.Size([64, 56, 56])
torch.Size([64, 3136])
torch.Size([196, 16, 64])
> step0: Inp | [196, 1, 16, 64]
> step1: torch.Size([256, 64]) 1
> step2: torch.Size([16, 16, 64])
> step3: Wgt | [[[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]]]
>>> Packed: 1024 to 1024
[196, 1, 16, 64] 16 1 [[16, 64]]
INFO:root:Running MatMul
222736.0 ->  0.10078552036199095 ms

 === Layer 4 : layer1.0.downsample.0 ===
torch.Size([64, 56, 56])
torch.Size([64, 3136])
torch.Size([196, 16, 64])
> step0: Inp | [196, 1, 16, 64]
> step1: torch.Size([256, 64]) 1
> step2: torch.Size([16, 16, 64])
> step3: Wgt | [[[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]]]
>>> Packed: 1024 to 1024
[196, 1, 16, 64] 16 1 [[16, 64]]
INFO:root:Running MatMul
222736.0 ->  0.10078552036199095 ms

 === Layer 5 : layer1.1.conv1 ===
torch.Size([256, 56, 56])
torch.Size([256, 3136])
torch.Size([196, 16, 256])
> step0: Inp | [196, 1, 16, 256]
> step1: torch.Size([64, 256]) 1
> step2: torch.Size([4, 16, 256])
> step3: Wgt | [[[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]]]
>>> Packed: 1024 to 1024
[196, 1, 16, 256] 4 1 [[16, 256]]
INFO:root:Running MatMul
206464.0 ->  0.09342262443438915 ms

 === Layer 6 : layer1.1.conv2 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) 1
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 576]], [[16, 576]], [[16, 576]], [[16, 576]]]
>>> Packed: 2304 to 2304
[183, 1, 16, 576] 4 1 [[16, 576]]
INFO:root:Running MatMul
427348.0 ->  0.19337013574660636 ms

 === Layer 7 : layer1.1.conv3 ===
torch.Size([64, 56, 56])
torch.Size([64, 3136])
torch.Size([196, 16, 64])
> step0: Inp | [196, 1, 16, 64]
> step1: torch.Size([256, 64]) 1
> step2: torch.Size([16, 16, 64])
> step3: Wgt | [[[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]]]
>>> Packed: 1024 to 1024
[196, 1, 16, 64] 16 1 [[16, 64]]
INFO:root:Running MatMul
222736.0 ->  0.10078552036199095 ms

 === Layer 8 : layer1.2.conv1 ===
torch.Size([256, 56, 56])
torch.Size([256, 3136])
torch.Size([196, 16, 256])
> step0: Inp | [196, 1, 16, 256]
> step1: torch.Size([64, 256]) 1
> step2: torch.Size([4, 16, 256])
> step3: Wgt | [[[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]]]
>>> Packed: 1024 to 1024
[196, 1, 16, 256] 4 1 [[16, 256]]
INFO:root:Running MatMul
206464.0 ->  0.09342262443438915 ms

 === Layer 9 : layer1.2.conv2 ===
torch.Size([64, 56, 56])
torch.Size([576, 2916])
torch.Size([183, 16, 576])
> step0: Inp | [183, 1, 16, 576]
> step1: torch.Size([64, 576]) 1
> step2: torch.Size([4, 16, 576])
> step3: Wgt | [[[16, 576]], [[16, 576]], [[16, 576]], [[16, 576]]]
>>> Packed: 2304 to 2304
[183, 1, 16, 576] 4 1 [[16, 576]]
INFO:root:Running MatMul
427348.0 ->  0.19337013574660636 ms

 === Layer 10 : layer1.2.conv3 ===
torch.Size([64, 56, 56])
torch.Size([64, 3136])
torch.Size([196, 16, 64])
> step0: Inp | [196, 1, 16, 64]
> step1: torch.Size([256, 64]) 1
> step2: torch.Size([16, 16, 64])
> step3: Wgt | [[[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]], [[16, 64]]]
>>> Packed: 1024 to 1024
[196, 1, 16, 64] 16 1 [[16, 64]]
INFO:root:Running MatMul
222736.0 ->  0.10078552036199095 ms

 === Layer 11 : layer2.0.conv1 ===
torch.Size([256, 56, 56])
torch.Size([256, 3136])
torch.Size([196, 16, 256])
> step0: Inp | [196, 1, 16, 256]
> step1: torch.Size([128, 256]) 1
> step2: torch.Size([8, 16, 256])
> step3: Wgt | [[[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]]]
>>> Packed: 2048 to 2048
[196, 1, 16, 256] 8 1 [[16, 256]]
INFO:root:Running MatMul
412656.0 ->  0.18672217194570137 ms

 === Layer 12 : layer2.0.conv2 ===
torch.Size([128, 56, 56])
torch.Size([1152, 729])
torch.Size([46, 16, 1152])
> step0: Inp | [46, 2, 16, 1024]
> step1: torch.Size([128, 1152]) 1
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]]]
>>> Packed: 9216 to 8192
[46, 2, 16, 1024] 8 2 [[16, 1024], [16, 128]]
INFO:root:Running MatMul
756391.0 ->  0.342258371040724 ms

 === Layer 13 : layer2.0.conv3 ===
torch.Size([128, 28, 28])
torch.Size([128, 784])
torch.Size([49, 16, 128])
> step0: Inp | [49, 1, 16, 128]
> step1: torch.Size([512, 128]) 1
> step2: torch.Size([32, 16, 128])
> step3: Wgt | [[[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]]]
>>> Packed: 4096 to 4096
[49, 1, 16, 128] 32 1 [[16, 128]]
INFO:root:Running MatMul
211824.0 ->  0.09584796380090499 ms

 === Layer 14 : layer2.0.downsample.0 ===
torch.Size([256, 56, 56])
torch.Size([256, 784])
torch.Size([49, 16, 256])
> step0: Inp | [49, 1, 16, 256]
> step1: torch.Size([512, 256]) 1
> step2: torch.Size([32, 16, 256])
> step3: Wgt | [[[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]]]
>>> Packed: 8192 to 8192
[49, 1, 16, 256] 32 1 [[16, 256]]
INFO:root:Running MatMul
412656.0 ->  0.18672217194570137 ms

 === Layer 15 : layer2.1.conv1 ===
torch.Size([512, 28, 28])
torch.Size([512, 784])
torch.Size([49, 16, 512])
> step0: Inp | [49, 1, 16, 512]
> step1: torch.Size([128, 512]) 1
> step2: torch.Size([8, 16, 512])
> step3: Wgt | [[[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]]]
>>> Packed: 4096 to 4096
[49, 1, 16, 512] 8 1 [[16, 512]]
INFO:root:Running MatMul
203976.0 ->  0.09229683257918553 ms

 === Layer 16 : layer2.1.conv2 ===
torch.Size([128, 28, 28])
torch.Size([1152, 676])
torch.Size([43, 16, 1152])
> step0: Inp | [43, 2, 16, 1024]
> step1: torch.Size([128, 1152]) 1
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]]]
>>> Packed: 9216 to 8192
[43, 2, 16, 1024] 8 2 [[16, 1024], [16, 128]]
INFO:root:Running MatMul
707071.0 ->  0.319941628959276 ms

 === Layer 17 : layer2.1.conv3 ===
torch.Size([128, 28, 28])
torch.Size([128, 784])
torch.Size([49, 16, 128])
> step0: Inp | [49, 1, 16, 128]
> step1: torch.Size([512, 128]) 1
> step2: torch.Size([32, 16, 128])
> step3: Wgt | [[[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]]]
>>> Packed: 4096 to 4096
[49, 1, 16, 128] 32 1 [[16, 128]]
INFO:root:Running MatMul
211824.0 ->  0.09584796380090499 ms

 === Layer 18 : layer2.2.conv1 ===
torch.Size([512, 28, 28])
torch.Size([512, 784])
torch.Size([49, 16, 512])
> step0: Inp | [49, 1, 16, 512]
> step1: torch.Size([128, 512]) 1
> step2: torch.Size([8, 16, 512])
> step3: Wgt | [[[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]]]
>>> Packed: 4096 to 4096
[49, 1, 16, 512] 8 1 [[16, 512]]
INFO:root:Running MatMul
203976.0 ->  0.09229683257918553 ms

 === Layer 19 : layer2.2.conv2 ===
torch.Size([128, 28, 28])
torch.Size([1152, 676])
torch.Size([43, 16, 1152])
> step0: Inp | [43, 2, 16, 1024]
> step1: torch.Size([128, 1152]) 1
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]]]
>>> Packed: 9216 to 8192
[43, 2, 16, 1024] 8 2 [[16, 1024], [16, 128]]
INFO:root:Running MatMul
707071.0 ->  0.319941628959276 ms

 === Layer 20 : layer2.2.conv3 ===
torch.Size([128, 28, 28])
torch.Size([128, 784])
torch.Size([49, 16, 128])
> step0: Inp | [49, 1, 16, 128]
> step1: torch.Size([512, 128]) 1
> step2: torch.Size([32, 16, 128])
> step3: Wgt | [[[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]]]
>>> Packed: 4096 to 4096
[49, 1, 16, 128] 32 1 [[16, 128]]
INFO:root:Running MatMul
211824.0 ->  0.09584796380090499 ms

 === Layer 21 : layer2.3.conv1 ===
torch.Size([512, 28, 28])
torch.Size([512, 784])
torch.Size([49, 16, 512])
> step0: Inp | [49, 1, 16, 512]
> step1: torch.Size([128, 512]) 1
> step2: torch.Size([8, 16, 512])
> step3: Wgt | [[[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]]]
>>> Packed: 4096 to 4096
[49, 1, 16, 512] 8 1 [[16, 512]]
INFO:root:Running MatMul
203976.0 ->  0.09229683257918553 ms

 === Layer 22 : layer2.3.conv2 ===
torch.Size([128, 28, 28])
torch.Size([1152, 676])
torch.Size([43, 16, 1152])
> step0: Inp | [43, 2, 16, 1024]
> step1: torch.Size([128, 1152]) 1
> step2: torch.Size([8, 16, 1152])
> step3: Wgt | [[[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]], [[16, 1024], [16, 128]]]
>>> Packed: 9216 to 8192
[43, 2, 16, 1024] 8 2 [[16, 1024], [16, 128]]
INFO:root:Running MatMul
707071.0 ->  0.319941628959276 ms

 === Layer 23 : layer2.3.conv3 ===
torch.Size([128, 28, 28])
torch.Size([128, 784])
torch.Size([49, 16, 128])
> step0: Inp | [49, 1, 16, 128]
> step1: torch.Size([512, 128]) 1
> step2: torch.Size([32, 16, 128])
> step3: Wgt | [[[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]], [[16, 128]]]
>>> Packed: 4096 to 4096
[49, 1, 16, 128] 32 1 [[16, 128]]
INFO:root:Running MatMul
211824.0 ->  0.09584796380090499 ms

 === Layer 24 : layer3.0.conv1 ===
torch.Size([512, 28, 28])
torch.Size([512, 784])
torch.Size([49, 16, 512])
> step0: Inp | [49, 1, 16, 512]
> step1: torch.Size([256, 512]) 1
> step2: torch.Size([16, 16, 512])
> step3: Wgt | [[[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]]]
>>> Packed: 8192 to 8192
[49, 1, 16, 512] 16 1 [[16, 512]]
INFO:root:Running MatMul
407424.0 ->  0.18435475113122174 ms

 === Layer 25 : layer3.0.conv2 ===
torch.Size([256, 28, 28])
torch.Size([2304, 169])
torch.Size([11, 16, 2304])
> step0: Inp | [11, 3, 16, 1024]
> step1: torch.Size([256, 2304]) 1
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]]]
>>> Packed: 36864 to 16384
[11, 3, 16, 1024] 16 3 [[16, 1024], [16, 1024], [16, 256]]
INFO:root:Running MatMul
543415.0 ->  0.24588914027149322 ms

 === Layer 26 : layer3.0.conv3 ===
torch.Size([256, 14, 14])
torch.Size([256, 196])
torch.Size([13, 16, 256])
> step0: Inp | [13, 1, 16, 256]
> step1: torch.Size([1024, 256]) 1
> step2: torch.Size([64, 16, 256])
> step3: Wgt | [[[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]]]
>>> Packed: 16384 to 16384
[13, 1, 16, 256] 64 1 [[16, 256]]
INFO:root:Running MatMul
219088.0 ->  0.09913484162895927 ms

 === Layer 27 : layer3.0.downsample.0 ===
torch.Size([512, 28, 28])
torch.Size([512, 196])
torch.Size([13, 16, 512])
> step0: Inp | [13, 1, 16, 512]
> step1: torch.Size([1024, 512]) 1
> step2: torch.Size([64, 16, 512])
> step3: Wgt | [[[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]]]
>>> Packed: 32768 to 32768
[13, 1, 16, 512] 64 1 [[16, 512]]
INFO:root:Running MatMul
432336.0 ->  0.19562714932126699 ms

 === Layer 28 : layer3.1.conv1 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 196])
torch.Size([13, 16, 1024])
> step0: Inp | [13, 1, 16, 1024]
> step1: torch.Size([256, 1024]) 1
> step2: torch.Size([16, 16, 1024])
> step3: Wgt | [[[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]]]
>>> Packed: 16384 to 16384
[13, 1, 16, 1024] 16 1 [[16, 1024]]
INFO:root:Running MatMul
215488.0 ->  0.09750588235294118 ms

 === Layer 29 : layer3.1.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) 1
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]]]
>>> Packed: 36864 to 16384
[9, 3, 16, 1024] 16 3 [[16, 1024], [16, 1024], [16, 256]]
INFO:root:Running MatMul
444663.0 ->  0.2012049773755656 ms

 === Layer 30 : layer3.1.conv3 ===
torch.Size([256, 14, 14])
torch.Size([256, 196])
torch.Size([13, 16, 256])
> step0: Inp | [13, 1, 16, 256]
> step1: torch.Size([1024, 256]) 1
> step2: torch.Size([64, 16, 256])
> step3: Wgt | [[[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]]]
>>> Packed: 16384 to 16384
[13, 1, 16, 256] 64 1 [[16, 256]]
INFO:root:Running MatMul
219088.0 ->  0.09913484162895927 ms

 === Layer 31 : layer3.2.conv1 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 196])
torch.Size([13, 16, 1024])
> step0: Inp | [13, 1, 16, 1024]
> step1: torch.Size([256, 1024]) 1
> step2: torch.Size([16, 16, 1024])
> step3: Wgt | [[[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]]]
>>> Packed: 16384 to 16384
[13, 1, 16, 1024] 16 1 [[16, 1024]]
INFO:root:Running MatMul
215488.0 ->  0.09750588235294118 ms

 === Layer 32 : layer3.2.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) 1
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]]]
>>> Packed: 36864 to 16384
[9, 3, 16, 1024] 16 3 [[16, 1024], [16, 1024], [16, 256]]
INFO:root:Running MatMul
444663.0 ->  0.2012049773755656 ms

 === Layer 33 : layer3.2.conv3 ===
torch.Size([256, 14, 14])
torch.Size([256, 196])
torch.Size([13, 16, 256])
> step0: Inp | [13, 1, 16, 256]
> step1: torch.Size([1024, 256]) 1
> step2: torch.Size([64, 16, 256])
> step3: Wgt | [[[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]]]
>>> Packed: 16384 to 16384
[13, 1, 16, 256] 64 1 [[16, 256]]
INFO:root:Running MatMul
219088.0 ->  0.09913484162895927 ms

 === Layer 34 : layer3.3.conv1 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 196])
torch.Size([13, 16, 1024])
> step0: Inp | [13, 1, 16, 1024]
> step1: torch.Size([256, 1024]) 1
> step2: torch.Size([16, 16, 1024])
> step3: Wgt | [[[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]]]
>>> Packed: 16384 to 16384
[13, 1, 16, 1024] 16 1 [[16, 1024]]
INFO:root:Running MatMul
215488.0 ->  0.09750588235294118 ms

 === Layer 35 : layer3.3.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) 1
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]]]
>>> Packed: 36864 to 16384
[9, 3, 16, 1024] 16 3 [[16, 1024], [16, 1024], [16, 256]]
INFO:root:Running MatMul
444663.0 ->  0.2012049773755656 ms

 === Layer 36 : layer3.3.conv3 ===
torch.Size([256, 14, 14])
torch.Size([256, 196])
torch.Size([13, 16, 256])
> step0: Inp | [13, 1, 16, 256]
> step1: torch.Size([1024, 256]) 1
> step2: torch.Size([64, 16, 256])
> step3: Wgt | [[[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]]]
>>> Packed: 16384 to 16384
[13, 1, 16, 256] 64 1 [[16, 256]]
INFO:root:Running MatMul
219088.0 ->  0.09913484162895927 ms

 === Layer 37 : layer3.4.conv1 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 196])
torch.Size([13, 16, 1024])
> step0: Inp | [13, 1, 16, 1024]
> step1: torch.Size([256, 1024]) 1
> step2: torch.Size([16, 16, 1024])
> step3: Wgt | [[[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]]]
>>> Packed: 16384 to 16384
[13, 1, 16, 1024] 16 1 [[16, 1024]]
INFO:root:Running MatMul
215488.0 ->  0.09750588235294118 ms

 === Layer 38 : layer3.4.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) 1
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]]]
>>> Packed: 36864 to 16384
[9, 3, 16, 1024] 16 3 [[16, 1024], [16, 1024], [16, 256]]
INFO:root:Running MatMul
444663.0 ->  0.2012049773755656 ms

 === Layer 39 : layer3.4.conv3 ===
torch.Size([256, 14, 14])
torch.Size([256, 196])
torch.Size([13, 16, 256])
> step0: Inp | [13, 1, 16, 256]
> step1: torch.Size([1024, 256]) 1
> step2: torch.Size([64, 16, 256])
> step3: Wgt | [[[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]]]
>>> Packed: 16384 to 16384
[13, 1, 16, 256] 64 1 [[16, 256]]
INFO:root:Running MatMul
219088.0 ->  0.09913484162895927 ms

 === Layer 40 : layer3.5.conv1 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 196])
torch.Size([13, 16, 1024])
> step0: Inp | [13, 1, 16, 1024]
> step1: torch.Size([256, 1024]) 1
> step2: torch.Size([16, 16, 1024])
> step3: Wgt | [[[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]]]
>>> Packed: 16384 to 16384
[13, 1, 16, 1024] 16 1 [[16, 1024]]
INFO:root:Running MatMul
215488.0 ->  0.09750588235294118 ms

 === Layer 41 : layer3.5.conv2 ===
torch.Size([256, 14, 14])
torch.Size([2304, 144])
torch.Size([9, 16, 2304])
> step0: Inp | [9, 3, 16, 1024]
> step1: torch.Size([256, 2304]) 1
> step2: torch.Size([16, 16, 2304])
> step3: Wgt | [[[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]], [[16, 1024], [16, 1024], [16, 256]]]
>>> Packed: 36864 to 16384
[9, 3, 16, 1024] 16 3 [[16, 1024], [16, 1024], [16, 256]]
INFO:root:Running MatMul
444663.0 ->  0.2012049773755656 ms

 === Layer 42 : layer3.5.conv3 ===
torch.Size([256, 14, 14])
torch.Size([256, 196])
torch.Size([13, 16, 256])
> step0: Inp | [13, 1, 16, 256]
> step1: torch.Size([1024, 256]) 1
> step2: torch.Size([64, 16, 256])
> step3: Wgt | [[[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]], [[16, 256]]]
>>> Packed: 16384 to 16384
[13, 1, 16, 256] 64 1 [[16, 256]]
INFO:root:Running MatMul
219088.0 ->  0.09913484162895927 ms

 === Layer 43 : layer4.0.conv1 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 196])
torch.Size([13, 16, 1024])
> step0: Inp | [13, 1, 16, 1024]
> step1: torch.Size([512, 1024]) 1
> step2: torch.Size([32, 16, 1024])
> step3: Wgt | [[[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]]]
>>> Packed: 32768 to 32768
[13, 1, 16, 1024] 32 1 [[16, 1024]]
INFO:root:Running MatMul
429936.0 ->  0.19454117647058825 ms

 === Layer 44 : layer4.0.conv2 ===
torch.Size([512, 14, 14])
torch.Size([4608, 36])
torch.Size([3, 16, 4608])
> step0: Inp | [3, 5, 16, 1024]
> step1: torch.Size([512, 4608]) 1
> step2: torch.Size([32, 16, 4608])
> step3: Wgt | [[[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]]]
>>> Packed: 147456 to 32768
[3, 5, 16, 1024] 32 5 [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]]
INFO:root:Running MatMul
494743.0 ->  0.22386561085972853 ms

 === Layer 45 : layer4.0.conv3 ===
torch.Size([512, 7, 7])
torch.Size([512, 49])
torch.Size([4, 16, 512])
> step0: Inp | [4, 1, 16, 512]
> step1: torch.Size([2048, 512]) 1
> step2: torch.Size([128, 16, 512])
> step3: Wgt | [[[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]]]
>>> Packed: 65536 to 65536
[4, 1, 16, 512] 128 1 [[16, 512]]
INFO:root:Running MatMul
266256.0 ->  0.12047782805429864 ms

 === Layer 46 : layer4.0.downsample.0 ===
torch.Size([1024, 14, 14])
torch.Size([1024, 49])
torch.Size([4, 16, 1024])
> step0: Inp | [4, 1, 16, 1024]
> step1: torch.Size([2048, 1024]) 1
> step2: torch.Size([128, 16, 1024])
> step3: Wgt | [[[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]], [[16, 1024]]]
>>> Packed: 131072 to 131072
[4, 1, 16, 1024] 128 1 [[16, 1024]]
INFO:root:Running MatMul
528912.0 ->  0.2393266968325792 ms

 === Layer 47 : layer4.1.conv1 ===
torch.Size([2048, 7, 7])
torch.Size([2048, 49])
torch.Size([4, 16, 2048])
> step0: Inp | [4, 2, 16, 1024]
> step1: torch.Size([512, 2048]) 1
> step2: torch.Size([32, 16, 2048])
> step3: Wgt | [[[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]]]
>>> Packed: 65536 to 32768
[4, 2, 16, 1024] 32 2 [[16, 1024], [16, 1024]]
INFO:root:Running MatMul
264976.0 ->  0.11989864253393666 ms

 === Layer 48 : layer4.1.conv2 ===
torch.Size([512, 7, 7])
torch.Size([4608, 25])
torch.Size([2, 16, 4608])
> step0: Inp | [2, 5, 16, 1024]
> step1: torch.Size([512, 4608]) 1
> step2: torch.Size([32, 16, 4608])
> step3: Wgt | [[[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]]]
>>> Packed: 147456 to 32768
[2, 5, 16, 1024] 32 5 [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]]
INFO:root:Running MatMul
330007.0 ->  0.14932443438914028 ms

 === Layer 49 : layer4.1.conv3 ===
torch.Size([512, 7, 7])
torch.Size([512, 49])
torch.Size([4, 16, 512])
> step0: Inp | [4, 1, 16, 512]
> step1: torch.Size([2048, 512]) 1
> step2: torch.Size([128, 16, 512])
> step3: Wgt | [[[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]]]
>>> Packed: 65536 to 65536
[4, 1, 16, 512] 128 1 [[16, 512]]
INFO:root:Running MatMul
266256.0 ->  0.12047782805429864 ms

 === Layer 50 : layer4.2.conv1 ===
torch.Size([2048, 7, 7])
torch.Size([2048, 49])
torch.Size([4, 16, 2048])
> step0: Inp | [4, 2, 16, 1024]
> step1: torch.Size([512, 2048]) 1
> step2: torch.Size([32, 16, 2048])
> step3: Wgt | [[[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]]]
>>> Packed: 65536 to 32768
[4, 2, 16, 1024] 32 2 [[16, 1024], [16, 1024]]
INFO:root:Running MatMul
264976.0 ->  0.11989864253393666 ms

 === Layer 51 : layer4.2.conv2 ===
torch.Size([512, 7, 7])
torch.Size([4608, 25])
torch.Size([2, 16, 4608])
> step0: Inp | [2, 5, 16, 1024]
> step1: torch.Size([512, 4608]) 1
> step2: torch.Size([32, 16, 4608])
> step3: Wgt | [[[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]], [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]]]
>>> Packed: 147456 to 32768
[2, 5, 16, 1024] 32 5 [[16, 1024], [16, 1024], [16, 1024], [16, 1024], [16, 512]]
INFO:root:Running MatMul
330007.0 ->  0.14932443438914028 ms

 === Layer 52 : layer4.2.conv3 ===
torch.Size([512, 7, 7])
torch.Size([512, 49])
torch.Size([4, 16, 512])
> step0: Inp | [4, 1, 16, 512]
> step1: torch.Size([2048, 512]) 1
> step2: torch.Size([128, 16, 512])
> step3: Wgt | [[[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]], [[16, 512]]]
>>> Packed: 65536 to 65536
[4, 1, 16, 512] 128 1 [[16, 512]]
INFO:root:Running MatMul
266256.0 ->  0.12047782805429864 ms

 === Layer 53 : fc ===
torch.Size([2048])
torch.Size([2048, 1])
torch.Size([1, 16, 2048])
> step0: Inp | [1, 2, 16, 1024]
> step1: torch.Size([1000, 2048]) 1
> step2: torch.Size([63, 16, 2048])
> step3: Wgt | [[[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]], [[16, 1024], [16, 1024]]]
>>> Packed: 129024 to 64512
[1, 2, 16, 1024] 63 2 [[16, 1024], [16, 1024]]
INFO:root:Running MatMul
130946.0 ->  0.059251583710407243 ms
The overall latecy is: 8.069402714932128
INFO:root:####################Finish######################
(env) (firesim) harry09119@FPGAserver:~/my_simulator$
